{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7IlOCD6FUSo"
      },
      "source": [
        "###1. Load model và chạy thử lấy output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-9yPVh27IXV"
      },
      "source": [
        "####TẢI DATA + GIẢI NÉN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALEzsdLvPFB4",
        "outputId": "914a649b-5b25-43e1-c692-44be2c2b5d6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-02 13:26:22--  http://cg.cs.tsinghua.edu.cn/download/DeepStab.zip\n",
            "Resolving cg.cs.tsinghua.edu.cn (cg.cs.tsinghua.edu.cn)... 101.6.15.70, 2402:f000:1:402:101:6:15:70\n",
            "Connecting to cg.cs.tsinghua.edu.cn (cg.cs.tsinghua.edu.cn)|101.6.15.70|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cg.cs.tsinghua.edu.cn/download/DeepStab.zip [following]\n",
            "--2024-09-02 13:26:23--  https://cg.cs.tsinghua.edu.cn/download/DeepStab.zip\n",
            "Connecting to cg.cs.tsinghua.edu.cn (cg.cs.tsinghua.edu.cn)|101.6.15.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8459488211 (7.9G) [application/zip]\n",
            "Saving to: ‘DeepStab.zip’\n",
            "\n",
            "DeepStab.zip        100%[===================>]   7.88G  8.17MB/s    in 17m 7s  \n",
            "\n",
            "2024-09-02 13:43:32 (7.85 MB/s) - ‘DeepStab.zip’ saved [8459488211/8459488211]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://cg.cs.tsinghua.edu.cn/download/DeepStab.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP48ruBZG2bg",
        "outputId": "73563f33-5035-4e68-f760-3e21754e7f07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/DeepStab.zip\n",
            "   creating: DeepStab/\n",
            "  inflating: DeepStab/Readme.txt     \n",
            "   creating: DeepStab/stable/\n",
            "  inflating: DeepStab/stable/1.avi   \n",
            "  inflating: DeepStab/stable/10.avi  \n",
            "  inflating: DeepStab/stable/11.avi  \n",
            "  inflating: DeepStab/stable/12.avi  \n",
            "  inflating: DeepStab/stable/13.avi  \n",
            "  inflating: DeepStab/stable/14.avi  \n",
            "  inflating: DeepStab/stable/15.avi  \n",
            "  inflating: DeepStab/stable/16.avi  \n",
            "  inflating: DeepStab/stable/17.avi  \n",
            "  inflating: DeepStab/stable/18.avi  \n",
            "  inflating: DeepStab/stable/19.avi  \n",
            "  inflating: DeepStab/stable/2.avi   \n",
            "  inflating: DeepStab/stable/20.avi  \n",
            "  inflating: DeepStab/stable/21.avi  \n",
            "  inflating: DeepStab/stable/22.avi  \n",
            "  inflating: DeepStab/stable/23.avi  \n",
            "  inflating: DeepStab/stable/24.avi  \n",
            "  inflating: DeepStab/stable/25.avi  \n",
            "  inflating: DeepStab/stable/26.avi  \n",
            "  inflating: DeepStab/stable/27.avi  \n",
            "  inflating: DeepStab/stable/28.avi  \n",
            "  inflating: DeepStab/stable/29.avi  \n",
            "  inflating: DeepStab/stable/3.avi   \n",
            "  inflating: DeepStab/stable/30.avi  \n",
            "  inflating: DeepStab/stable/31.avi  \n",
            "  inflating: DeepStab/stable/32.avi  \n",
            "  inflating: DeepStab/stable/33.avi  \n",
            "  inflating: DeepStab/stable/34.avi  \n",
            "  inflating: DeepStab/stable/35.avi  \n",
            "  inflating: DeepStab/stable/36.avi  \n",
            "  inflating: DeepStab/stable/37.avi  \n",
            "  inflating: DeepStab/stable/38.avi  \n",
            "  inflating: DeepStab/stable/39.avi  \n",
            "  inflating: DeepStab/stable/4.avi   \n",
            "  inflating: DeepStab/stable/40.avi  \n",
            "  inflating: DeepStab/stable/41.avi  \n",
            "  inflating: DeepStab/stable/42.avi  \n",
            "  inflating: DeepStab/stable/43.avi  \n",
            "  inflating: DeepStab/stable/44.avi  \n",
            "  inflating: DeepStab/stable/45.avi  \n",
            "  inflating: DeepStab/stable/46.avi  \n",
            "  inflating: DeepStab/stable/47.avi  \n",
            "  inflating: DeepStab/stable/48.avi  \n",
            "  inflating: DeepStab/stable/49.avi  \n",
            "  inflating: DeepStab/stable/5.avi   \n",
            "  inflating: DeepStab/stable/50.avi  \n",
            "  inflating: DeepStab/stable/51.avi  \n",
            "  inflating: DeepStab/stable/52.avi  \n",
            "  inflating: DeepStab/stable/53.avi  \n",
            "  inflating: DeepStab/stable/54.avi  \n",
            "  inflating: DeepStab/stable/55.avi  \n",
            "  inflating: DeepStab/stable/56.avi  \n",
            "  inflating: DeepStab/stable/57.avi  \n",
            "  inflating: DeepStab/stable/58.avi  \n",
            "  inflating: DeepStab/stable/59.avi  \n",
            "  inflating: DeepStab/stable/6.avi   \n",
            "  inflating: DeepStab/stable/60.avi  \n",
            "  inflating: DeepStab/stable/61.avi  \n",
            "  inflating: DeepStab/stable/7.avi   \n",
            "  inflating: DeepStab/stable/8.avi   \n",
            "  inflating: DeepStab/stable/9.avi   \n",
            "   creating: DeepStab/unstable/\n",
            "  inflating: DeepStab/unstable/1.avi  \n",
            "  inflating: DeepStab/unstable/10.avi  \n",
            "  inflating: DeepStab/unstable/11.avi  \n",
            "  inflating: DeepStab/unstable/12.avi  \n",
            "  inflating: DeepStab/unstable/13.avi  \n",
            "  inflating: DeepStab/unstable/14.avi  \n",
            "  inflating: DeepStab/unstable/15.avi  \n",
            "  inflating: DeepStab/unstable/16.avi  \n",
            "  inflating: DeepStab/unstable/17.avi  \n",
            "  inflating: DeepStab/unstable/18.avi  \n",
            "  inflating: DeepStab/unstable/19.avi  \n",
            "  inflating: DeepStab/unstable/2.avi  \n",
            "  inflating: DeepStab/unstable/20.avi  \n",
            "  inflating: DeepStab/unstable/21.avi  \n",
            "  inflating: DeepStab/unstable/22.avi  \n",
            "  inflating: DeepStab/unstable/23.avi  \n",
            "  inflating: DeepStab/unstable/24.avi  \n",
            "  inflating: DeepStab/unstable/25.avi  \n",
            "  inflating: DeepStab/unstable/26.avi  \n",
            "  inflating: DeepStab/unstable/27.avi  \n",
            "  inflating: DeepStab/unstable/28.avi  \n",
            "  inflating: DeepStab/unstable/29.avi  \n",
            "  inflating: DeepStab/unstable/3.avi  \n",
            "  inflating: DeepStab/unstable/30.avi  \n",
            "  inflating: DeepStab/unstable/31.avi  \n",
            "  inflating: DeepStab/unstable/32.avi  \n",
            "  inflating: DeepStab/unstable/33.avi  \n",
            "  inflating: DeepStab/unstable/34.avi  \n",
            "  inflating: DeepStab/unstable/35.avi  \n",
            "  inflating: DeepStab/unstable/36.avi  \n",
            "  inflating: DeepStab/unstable/37.avi  \n",
            "  inflating: DeepStab/unstable/38.avi  \n",
            "  inflating: DeepStab/unstable/39.avi  \n",
            "  inflating: DeepStab/unstable/4.avi  \n",
            "  inflating: DeepStab/unstable/40.avi  \n",
            "  inflating: DeepStab/unstable/41.avi  \n",
            "  inflating: DeepStab/unstable/42.avi  \n",
            "  inflating: DeepStab/unstable/43.avi  \n",
            "  inflating: DeepStab/unstable/44.avi  \n",
            "  inflating: DeepStab/unstable/45.avi  \n",
            "  inflating: DeepStab/unstable/46.avi  \n",
            "  inflating: DeepStab/unstable/47.avi  \n",
            "  inflating: DeepStab/unstable/48.avi  \n",
            "  inflating: DeepStab/unstable/49.avi  \n",
            "  inflating: DeepStab/unstable/5.avi  \n",
            "  inflating: DeepStab/unstable/50.avi  \n",
            "  inflating: DeepStab/unstable/51.avi  \n",
            "  inflating: DeepStab/unstable/52.avi  \n",
            "  inflating: DeepStab/unstable/53.avi  \n",
            "  inflating: DeepStab/unstable/54.avi  \n",
            "  inflating: DeepStab/unstable/55.avi  \n",
            "  inflating: DeepStab/unstable/56.avi  \n",
            "  inflating: DeepStab/unstable/57.avi  \n",
            "  inflating: DeepStab/unstable/58.avi  \n",
            "  inflating: DeepStab/unstable/59.avi  \n",
            "  inflating: DeepStab/unstable/6.avi  \n",
            "  inflating: DeepStab/unstable/60.avi  \n",
            "  inflating: DeepStab/unstable/61.avi  \n",
            "  inflating: DeepStab/unstable/7.avi  \n",
            "  inflating: DeepStab/unstable/8.avi  \n",
            "  inflating: DeepStab/unstable/9.avi  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/DeepStab.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC6RqX3X7yr4"
      },
      "source": [
        "####MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orfK7shtFlJW"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels=3, depth=3):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Shuffle pixels to expand in channel dimension\n",
        "        # shuffler_list = [PixelShuffle(0.5) for i in range(depth)]\n",
        "        # self.shuffler = nn.Sequential(*shuffler_list)\n",
        "        self.shuffler = PixelShuffle(1 / 2**depth)\n",
        "\n",
        "        relu = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "        # FF_RCAN or FF_Resblocks\n",
        "        self.interpolate = Interpolation(5, 12, in_channels * (4**depth), act=relu)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Encoder: Shuffle-spread --> Feature Fusion --> Return fused features\n",
        "        \"\"\"\n",
        "        feats1 = self.shuffler(x1)\n",
        "        feats2 = self.shuffler(x2)\n",
        "\n",
        "        feats = self.interpolate(feats1, feats2)\n",
        "\n",
        "        return feats\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, depth=3):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # shuffler_list = [PixelShuffle(2) for i in range(depth)]\n",
        "        # self.shuffler = nn.Sequential(*shuffler_list)\n",
        "        self.shuffler = PixelShuffle(2**depth)\n",
        "\n",
        "    def forward(self, feats):\n",
        "        out = self.shuffler(feats)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CAIN(nn.Module):\n",
        "    def __init__(self,training, depth=3):\n",
        "        super(CAIN, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(in_channels=3, depth=depth)\n",
        "        self.decoder = Decoder(depth=depth)\n",
        "        self.training = training\n",
        "    def forward(self, x1, x2):\n",
        "        x1, m1 = sub_mean(x1)\n",
        "        x2, m2 = sub_mean(x2)\n",
        "\n",
        "        if not self.training:\n",
        "            paddingInput, paddingOutput = InOutPaddings(x1)\n",
        "            x1 = paddingInput(x1)\n",
        "            x2 = paddingInput(x2)\n",
        "\n",
        "        feats = self.encoder(x1, x2)\n",
        "        out = self.decoder(feats)\n",
        "\n",
        "        if not self.training:\n",
        "            out = paddingOutput(out)\n",
        "\n",
        "        mi = (m1 + m2) / 2\n",
        "        out += mi\n",
        "\n",
        "        return out, feats\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def sub_mean(x):\n",
        "    mean = x.mean(2, keepdim=True).mean(3, keepdim=True)\n",
        "    x -= mean\n",
        "    return x, mean\n",
        "\n",
        "def InOutPaddings(x):\n",
        "    w, h = x.size(3), x.size(2)\n",
        "    padding_width, padding_height = 0, 0\n",
        "    if w != ((w >> 7) << 7):\n",
        "        padding_width = (((w >> 7) + 1) << 7) - w\n",
        "    if h != ((h >> 7) << 7):\n",
        "        padding_height = (((h >> 7) + 1) << 7) - h\n",
        "    paddingInput = nn.ReflectionPad2d(padding=[padding_width // 2, padding_width - padding_width // 2,\n",
        "                                               padding_height // 2, padding_height - padding_height // 2])\n",
        "    paddingOutput = nn.ReflectionPad2d(padding=[0 - padding_width // 2, padding_width // 2 - padding_width,\n",
        "                                                0 - padding_height // 2, padding_height // 2 - padding_height])\n",
        "    return paddingInput, paddingOutput\n",
        "\n",
        "\n",
        "class ConvNorm(nn.Module):\n",
        "    def __init__(self, in_feat, out_feat, kernel_size, stride=1, norm=False):\n",
        "        super(ConvNorm, self).__init__()\n",
        "\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv = nn.Conv2d(in_feat, out_feat, stride=stride, kernel_size=kernel_size, bias=True)\n",
        "\n",
        "        self.norm = norm\n",
        "        if norm == 'IN':\n",
        "            self.norm = nn.InstanceNorm2d(out_feat, track_running_stats=True)\n",
        "        elif norm == 'BN':\n",
        "            self.norm = nn.BatchNorm2d(out_feat)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv(out)\n",
        "        if self.norm:\n",
        "            out = self.norm(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UpConvNorm(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, mode='transpose', norm=False):\n",
        "        super(UpConvNorm, self).__init__()\n",
        "\n",
        "        if mode == 'transpose':\n",
        "            self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "        elif mode == 'shuffle':\n",
        "            self.upconv = nn.Sequential(\n",
        "                ConvNorm(in_channels, 4*out_channels, kernel_size=3, stride=1, norm=norm),\n",
        "                PixelShuffle(2))\n",
        "        else:\n",
        "            # out_channels is always going to be the same as in_channels\n",
        "            self.upconv = nn.Sequential(\n",
        "                nn.Upsample(mode='bilinear', scale_factor=2, align_corners=False),\n",
        "                ConvNorm(in_channels, out_channels, kernel_size=1, stride=1, norm=norm))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.upconv(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class meanShift(nn.Module):\n",
        "    def __init__(self, rgbRange, rgbMean, sign, nChannel=3):\n",
        "        super(meanShift, self).__init__()\n",
        "        if nChannel == 1:\n",
        "            l = rgbMean[0] * rgbRange * float(sign)\n",
        "\n",
        "            self.shifter = nn.Conv2d(1, 1, kernel_size=1, stride=1, padding=0)\n",
        "            self.shifter.weight.data = torch.eye(1).view(1, 1, 1, 1)\n",
        "            self.shifter.bias.data = torch.Tensor([l])\n",
        "        elif nChannel == 3:\n",
        "            r = rgbMean[0] * rgbRange * float(sign)\n",
        "            g = rgbMean[1] * rgbRange * float(sign)\n",
        "            b = rgbMean[2] * rgbRange * float(sign)\n",
        "\n",
        "            self.shifter = nn.Conv2d(3, 3, kernel_size=1, stride=1, padding=0)\n",
        "            self.shifter.weight.data = torch.eye(3).view(3, 3, 1, 1)\n",
        "            self.shifter.bias.data = torch.Tensor([r, g, b])\n",
        "        else:\n",
        "            r = rgbMean[0] * rgbRange * float(sign)\n",
        "            g = rgbMean[1] * rgbRange * float(sign)\n",
        "            b = rgbMean[2] * rgbRange * float(sign)\n",
        "            self.shifter = nn.Conv2d(6, 6, kernel_size=1, stride=1, padding=0)\n",
        "            self.shifter.weight.data = torch.eye(6).view(6, 6, 1, 1)\n",
        "            self.shifter.bias.data = torch.Tensor([r, g, b, r, g, b])\n",
        "\n",
        "        # Freeze the meanShift layer\n",
        "        for params in self.shifter.parameters():\n",
        "            params.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shifter(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\" CONV - (BN) - RELU - CONV - (BN) \"\"\"\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_feat, out_feat, kernel_size=3, reduction=False, bias=True, # 'reduction' is just for placeholder\n",
        "                 norm=False, act=nn.ReLU(True), downscale=False):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.body = nn.Sequential(\n",
        "            ConvNorm(in_feat, out_feat, kernel_size=kernel_size, stride=2 if downscale else 1),\n",
        "            act,\n",
        "            ConvNorm(out_feat, out_feat, kernel_size=kernel_size, stride=1)\n",
        "        )\n",
        "\n",
        "        self.downscale = None\n",
        "        if downscale:\n",
        "            self.downscale = nn.Conv2d(in_feat, out_feat, kernel_size=1, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        out = self.body(x)\n",
        "        if self.downscale is not None:\n",
        "            res = self.downscale(res)\n",
        "        out += res\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "## Channel Attention (CA) Layer\n",
        "class CALayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(CALayer, self).__init__()\n",
        "        # global average pooling: feature --> point\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        # feature channel downscale and upscale --> channel weight\n",
        "        self.conv_du = nn.Sequential(\n",
        "            nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.conv_du(y)\n",
        "        return x * y, y\n",
        "\n",
        "\n",
        "## Residual Channel Attention Block (RCAB)\n",
        "class RCAB(nn.Module):\n",
        "    def __init__(self, in_feat, out_feat, kernel_size, reduction, bias=True,\n",
        "            norm=False, act=nn.ReLU(True), downscale=False, return_ca=False):\n",
        "        super(RCAB, self).__init__()\n",
        "\n",
        "        self.body = nn.Sequential(\n",
        "            ConvNorm(in_feat, out_feat, kernel_size, stride=2 if downscale else 1, norm=norm),\n",
        "            act,\n",
        "            ConvNorm(out_feat, out_feat, kernel_size, stride=1, norm=norm),\n",
        "            CALayer(out_feat, reduction)\n",
        "        )\n",
        "        self.downscale = downscale\n",
        "        if downscale:\n",
        "            self.downConv = nn.Conv2d(in_feat, out_feat, kernel_size=3, stride=2, padding=1)\n",
        "        self.return_ca = return_ca\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        out, ca = self.body(x)\n",
        "        if self.downscale:\n",
        "            res = self.downConv(res)\n",
        "        out += res\n",
        "\n",
        "        if self.return_ca:\n",
        "            return out, ca\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "## Residual Group (RG)\n",
        "class ResidualGroup(nn.Module):\n",
        "    def __init__(self, Block, n_resblocks, n_feat, kernel_size, reduction, act, norm=False):\n",
        "        super(ResidualGroup, self).__init__()\n",
        "\n",
        "        modules_body = [Block(n_feat, n_feat, kernel_size, reduction, bias=True, norm=norm, act=act)\n",
        "            for _ in range(n_resblocks)]\n",
        "        modules_body.append(ConvNorm(n_feat, n_feat, kernel_size, stride=1, norm=norm))\n",
        "        self.body = nn.Sequential(*modules_body)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.body(x)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "\n",
        "def pixel_shuffle(input, scale_factor):\n",
        "    batch_size, channels, in_height, in_width = input.size()\n",
        "\n",
        "    out_channels = int(int(channels / scale_factor) / scale_factor)\n",
        "    out_height = int(in_height * scale_factor)\n",
        "    out_width = int(in_width * scale_factor)\n",
        "\n",
        "    if scale_factor >= 1:\n",
        "        input_view = input.contiguous().view(batch_size, out_channels, scale_factor, scale_factor, in_height, in_width)\n",
        "        shuffle_out = input_view.permute(0, 1, 4, 2, 5, 3).contiguous()\n",
        "    else:\n",
        "        block_size = int(1 / scale_factor)\n",
        "        input_view = input.contiguous().view(batch_size, channels, out_height, block_size, out_width, block_size)\n",
        "        shuffle_out = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()\n",
        "\n",
        "    return shuffle_out.view(batch_size, out_channels, out_height, out_width)\n",
        "\n",
        "\n",
        "class PixelShuffle(nn.Module):\n",
        "    def __init__(self, scale_factor):\n",
        "        super(PixelShuffle, self).__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        return pixel_shuffle(x, self.scale_factor)\n",
        "    def extra_repr(self):\n",
        "        return 'scale_factor={}'.format(self.scale_factor)\n",
        "\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size,\n",
        "         stride=1, bias=True, groups=1):\n",
        "    return nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=kernel_size,\n",
        "        padding=kernel_size//2,\n",
        "        stride=1,\n",
        "        bias=bias,\n",
        "        groups=groups)\n",
        "\n",
        "\n",
        "def conv1x1(in_channels, out_channels, stride=1, bias=True, groups=1):\n",
        "    return nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=1,\n",
        "        stride=stride,\n",
        "        bias=bias,\n",
        "        groups=groups)\n",
        "\n",
        "def conv3x3(in_channels, out_channels, stride=1,\n",
        "            padding=1, bias=True, groups=1):\n",
        "    return nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=padding,\n",
        "        bias=bias,\n",
        "        groups=groups)\n",
        "\n",
        "def conv5x5(in_channels, out_channels, stride=1,\n",
        "            padding=2, bias=True, groups=1):\n",
        "    return nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=5,\n",
        "        stride=stride,\n",
        "        padding=padding,\n",
        "        bias=bias,\n",
        "        groups=groups)\n",
        "\n",
        "def conv7x7(in_channels, out_channels, stride=1,\n",
        "            padding=3, bias=True, groups=1):\n",
        "    return nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=7,\n",
        "        stride=stride,\n",
        "        padding=padding,\n",
        "        bias=bias,\n",
        "        groups=groups)\n",
        "\n",
        "def upconv2x2(in_channels, out_channels, mode='shuffle'):\n",
        "    if mode == 'transpose':\n",
        "        return nn.ConvTranspose2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=4,\n",
        "            stride=2,\n",
        "            padding=1)\n",
        "    elif mode == 'shuffle':\n",
        "        return nn.Sequential(\n",
        "            conv3x3(in_channels, 4*out_channels),\n",
        "            PixelShuffle(2))\n",
        "    else:\n",
        "        # out_channels is always going to be the same as in_channels\n",
        "        return nn.Sequential(\n",
        "            nn.Upsample(mode='bilinear', scale_factor=2, align_corners=False),\n",
        "            conv1x1(in_channels, out_channels))\n",
        "\n",
        "\n",
        "\n",
        "class Interpolation(nn.Module):\n",
        "    def __init__(self, n_resgroups, n_resblocks, n_feats,\n",
        "                 reduction=16, act=nn.LeakyReLU(0.2, True), norm=False):\n",
        "        super(Interpolation, self).__init__()\n",
        "\n",
        "        # define modules: head, body, tail\n",
        "        self.headConv = conv3x3(n_feats * 2, n_feats)\n",
        "\n",
        "        modules_body = [\n",
        "            ResidualGroup(\n",
        "                RCAB,\n",
        "                n_resblocks=n_resblocks,\n",
        "                n_feat=n_feats,\n",
        "                kernel_size=3,\n",
        "                reduction=reduction,\n",
        "                act=act,\n",
        "                norm=norm)\n",
        "            for _ in range(n_resgroups)]\n",
        "        self.body = nn.Sequential(*modules_body)\n",
        "\n",
        "        self.tailConv = conv3x3(n_feats, n_feats)\n",
        "\n",
        "    def forward(self, x0, x1):\n",
        "        # Build input tensor\n",
        "        x = torch.cat([x0, x1], dim=1)\n",
        "        x = self.headConv(x)\n",
        "\n",
        "        res = self.body(x)\n",
        "        res += x\n",
        "\n",
        "        out = self.tailConv(res)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Interpolation_res(nn.Module):\n",
        "    def __init__(self, n_resgroups, n_resblocks, n_feats,\n",
        "                 act=nn.LeakyReLU(0.2, True), norm=False):\n",
        "        super(Interpolation_res, self).__init__()\n",
        "\n",
        "        # define modules: head, body, tail (reduces concatenated inputs to n_feat)\n",
        "        self.headConv = conv3x3(n_feats * 2, n_feats)\n",
        "\n",
        "        modules_body = [ResidualGroup(ResBlock, n_resblocks=n_resblocks, n_feat=n_feats, kernel_size=3,\n",
        "                            reduction=0, act=act, norm=norm)\n",
        "                        for _ in range(n_resgroups)]\n",
        "        self.body = nn.Sequential(*modules_body)\n",
        "\n",
        "        self.tailConv = conv3x3(n_feats, n_feats)\n",
        "\n",
        "    def forward(self, x0, x1):\n",
        "        # Build input tensor\n",
        "        x = torch.cat([x0, x1], dim=1)\n",
        "        x = self.headConv(x)\n",
        "\n",
        "        res = x\n",
        "        for m in self.body:\n",
        "            res = m(res)\n",
        "        res += x\n",
        "\n",
        "        x = self.tailConv(res)\n",
        "\n",
        "        return x\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction_ratio=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels // reduction_ratio),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_channels // reduction_ratio, in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_pool = self.avg_pool(x).view(x.size(0), -1)\n",
        "        channel_att = self.fc(avg_pool).view(x.size(0), x.size(1), 1, 1)\n",
        "        return x * channel_att\n",
        "\n",
        "class ResidualBlockWithChannelAttention(nn.Module):\n",
        "    def __init__(self, in_features=32, out_features=32):\n",
        "        super(ResidualBlockWithChannelAttention, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_features, out_features, kernel_size=1, stride=1, padding=0),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.channel_attention = ChannelAttention(out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv(x)\n",
        "        x1 = self.channel_attention(x1)\n",
        "        return x1 + x\n",
        "\n",
        "class ConvBlockWithChannelAttention(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(ConvBlockWithChannelAttention, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_features, out_features, kernel_size=1, stride=1, padding=0),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "        self.channel_attention = ChannelAttention(out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv(x)\n",
        "        x1 = self.channel_attention(x1)\n",
        "        return x1\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv_block = ConvBlockWithChannelAttention(15, hidden_channels)\n",
        "        self.res_block1 = ResidualBlockWithChannelAttention(hidden_channels, hidden_channels)\n",
        "        self.res_block2 = ResidualBlockWithChannelAttention(hidden_channels, hidden_channels)\n",
        "        self.res_block3 = ResidualBlockWithChannelAttention(hidden_channels, hidden_channels)\n",
        "        self.res_block4 = ResidualBlockWithChannelAttention(hidden_channels, hidden_channels)\n",
        "        self.res_block5 = ResidualBlockWithChannelAttention(hidden_channels, hidden_channels)\n",
        "        self.conv_final = nn.Conv2d(hidden_channels, 3, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        # Initialize the weights with Kaiming initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        total_trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        print(f\"Total ResNet Parameters: {total_trainable_params}\")\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ConvBlock\n",
        "        x = self.conv_block(x)\n",
        "\n",
        "        # Residual Block 1\n",
        "        x = self.res_block1(x)\n",
        "\n",
        "        # Residual Block 2\n",
        "        x = self.res_block2(x)\n",
        "\n",
        "        # Residual Block 3\n",
        "        x = self.res_block3(x)\n",
        "\n",
        "        # Residual Block 4\n",
        "        x = self.res_block4(x)\n",
        "\n",
        "        # Residual Block 5\n",
        "        x = self.res_block5(x)\n",
        "\n",
        "        # Final Convolutional Layer\n",
        "        x = self.conv_final(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Get_gradient(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Get_gradient, self).__init__()\n",
        "        kernel_v = [[0, -1, 0],\n",
        "                    [0, 0, 0],\n",
        "                    [0, 1, 0]]\n",
        "        kernel_h = [[0, 0, 0],\n",
        "                    [-1, 0, 1],\n",
        "                    [0, 0, 0]]\n",
        "        kernel_h = torch.FloatTensor(kernel_h).unsqueeze(0).unsqueeze(0)\n",
        "        kernel_v = torch.FloatTensor(kernel_v).unsqueeze(0).unsqueeze(0)\n",
        "        self.weight_h = nn.Parameter(data = kernel_h, requires_grad = False).cuda()\n",
        "        self.weight_v = nn.Parameter(data = kernel_v, requires_grad = False).cuda()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x[:, 0]\n",
        "        x1 = x[:, 1]\n",
        "        x2 = x[:, 2]\n",
        "        x0_v = F.conv2d(x0.unsqueeze(1), self.weight_v, padding=2)\n",
        "        x0_h = F.conv2d(x0.unsqueeze(1), self.weight_h, padding=2)\n",
        "\n",
        "        x1_v = F.conv2d(x1.unsqueeze(1), self.weight_v, padding=2)\n",
        "        x1_h = F.conv2d(x1.unsqueeze(1), self.weight_h, padding=2)\n",
        "\n",
        "        x2_v = F.conv2d(x2.unsqueeze(1), self.weight_v, padding=2)\n",
        "        x2_h = F.conv2d(x2.unsqueeze(1), self.weight_h, padding=2)\n",
        "\n",
        "        x0 = torch.sqrt(torch.pow(x0_v, 2) + torch.pow(x0_h, 2) + 1e-6)\n",
        "        x1 = torch.sqrt(torch.pow(x1_v, 2) + torch.pow(x1_h, 2) + 1e-6)\n",
        "        x2 = torch.sqrt(torch.pow(x2_v, 2) + torch.pow(x2_h, 2) + 1e-6)\n",
        "\n",
        "        x = torch.cat([x0, x1, x2], dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKaNJb8UF2C7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "H,W = 256,256\n",
        "device = 'cuda'\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Video Stabilization using CAIN')\n",
        "    parser.add_argument('--in_path', type=str, help='Input video file path')\n",
        "    parser.add_argument('--out_path', type=str, help='Output stabilized video file path')\n",
        "    return parser.parse_args()\n",
        "\n",
        "def save_video(frames, path):\n",
        "    frame_count,h,w,_ = frames.shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(path, fourcc, 30.0, (w,h))\n",
        "    for idx in range(frame_count):\n",
        "        out.write(frames[idx,...])\n",
        "    out.release()\n",
        "\n",
        "def stabilize(in_path,out_path):\n",
        "\n",
        "    if not os.path.exists(in_path):\n",
        "        print(f\"The input file '{in_path}' does not exist.\")\n",
        "        exit()\n",
        "    _,ext = os.path.splitext(in_path)\n",
        "    if ext not in ['.mp4','.avi']:\n",
        "        print(f\"The input file '{in_path}' is not a supported video file (only .mp4 and .avi are supported).\")\n",
        "        exit()\n",
        "\n",
        "    #Load frames and stardardize\n",
        "    cap = cv2.VideoCapture(in_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frames = np.zeros((frame_count,H,W,3),np.float32)\n",
        "    for i in range(frame_count):\n",
        "        ret,img = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        img = cv2.resize(img,(W,H))\n",
        "        img = ((img / 255.0) * 2) - 1\n",
        "        frames[i,...] = img\n",
        "        print(frames.size)\n",
        "    # stabilize video\n",
        "    SKIP = 1\n",
        "    ITER = 3\n",
        "    interpolated = frames.copy()\n",
        "    for iter in range(ITER):\n",
        "        print(iter)\n",
        "        temp = interpolated.copy()\n",
        "        for frame_idx in range(2,frame_count - 2):\n",
        "            torch.cuda.empty_cache()\n",
        "            ft_minus = torch.from_numpy(interpolated[frame_idx - 1,...]).permute(2,0,1).unsqueeze(0).to(device)\n",
        "            ft = torch.from_numpy(frames[frame_idx]).permute(2,0,1).unsqueeze(0).to(device)\n",
        "            ft_plus = torch.from_numpy(interpolated[frame_idx + 1,...]).permute(2,0,1).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                fout,features = cain(ft_minus,ft_plus)\n",
        "                #refinement step\n",
        "                if iter == 2:\n",
        "                    ft_2 = torch.from_numpy(frames[frame_idx -2,...]).permute(2,0,1).unsqueeze(0).to(device)\n",
        "                    ft_1 = torch.from_numpy(frames[frame_idx -1,...]).permute(2,0,1).unsqueeze(0).to(device)\n",
        "                    ftplus1 = torch.from_numpy(frames[frame_idx +1,...]).permute(2,0,1).unsqueeze(0).to(device)\n",
        "                    ftplus2 = torch.from_numpy(frames[frame_idx +2,...]).permute(2,0,1).unsqueeze(0).to(device)\n",
        "                    resnet_input = torch.cat([ft_2, ft_1, fout, ftplus1, ftplus2],dim = 1)\n",
        "                    fout = resnet(resnet_input)\n",
        "            temp[frame_idx,...] = fout.cpu().squeeze(0).permute(1,2,0).numpy()\n",
        "            img  = (((fout.cpu().squeeze(0).permute(1,2,0).numpy() + 1) / 2)*255.0)\n",
        "            img  = np.clip(img,0,255).astype(np.uint8)\n",
        "        interpolated = temp.copy()\n",
        "    stable_frames = np.clip((255 *(interpolated + 1) / 2),0,255).astype(np.uint8)\n",
        "    save_video(stable_frames,out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH4SYdcrGpvj",
        "outputId": "fc6d1eb2-d489-432e-ac0d-4568e3bc7b45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1GJY31u3JMGansi0glQW1YHLKpPFQqlnO\n",
            "From (redirected): https://drive.google.com/uc?id=1GJY31u3JMGansi0glQW1YHLKpPFQqlnO&confirm=t&uuid=8335219c-c0bf-40a4-8512-08036d64c1c9\n",
            "To: /content/ckpts.rar\n",
            "100% 160M/160M [00:02<00:00, 71.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# tải trọng số\n",
        "\n",
        "!gdown --id 1GJY31u3JMGansi0glQW1YHLKpPFQqlnO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUyu7wCCHWNh",
        "outputId": "a450adc8-2fa3-4c76-8dcc-4e58a508fd10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/ckpts.rar\n",
            "\n",
            "Creating    ckpts                                                     OK\n",
            "Creating    ckpts/CAIN                                                OK\n",
            "Extracting  ckpts/CAIN/pretrained_cain.pth                               \b\b\b\b  2%\b\b\b\b  5%\b\b\b\b  7%\b\b\b\b 10%\b\b\b\b 13%\b\b\b\b 15%\b\b\b\b 18%\b\b\b\b 20%\b\b\b\b 23%\b\b\b\b 26%\b\b\b\b 28%\b\b\b\b 31%\b\b\b\b 34%\b\b\b\b 36%\b\b\b\b 39%\b\b\b\b 41%\b\b\b\b 44%\b\b\b\b 47%\b\b\b\b 49%\b\b\b\b 52%\b\b\b\b 54%\b\b\b\b 57%\b\b\b\b 60%\b\b\b\b 62%\b\b\b\b 65%\b\b\b\b 68%\b\b\b\b 70%\b\b\b\b 73%\b\b\b\b 75%\b\b\b\b 78%\b\b\b\b 81%\b\b\b\b 83%\b\b\b\b 86%\b\b\b\b 88%\b\b\b\b 91%\b\b\b\b 94%\b\b\b\b 96%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "Creating    ckpts/ResNet                                              OK\n",
            "Extracting  ckpts/ResNet/resnet_5.pth                                    \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ],
      "source": [
        "#giải nén rar file\n",
        "\n",
        "!unrar x /content/ckpts.rar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk55-K9VGRuY",
        "outputId": "0daf83fa-6710-4cfd-8394-11064a9f49ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total ResNet Parameters: 25499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-3ab71ed9747d>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  cain_ckpt = torch.load('/content/ckpts/CAIN/pretrained_cain.pth')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained CAIN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-3ab71ed9747d>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/ckpts/ResNet/resnet_5.pth')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet = ResNet(hidden_channels=64)\n",
        "resnet = torch.nn.DataParallel(resnet).to(device).eval()\n",
        "cain = CAIN(training= False,depth=3)\n",
        "cain = torch.nn.DataParallel(cain).to(device).eval()\n",
        "cain_ckpt = torch.load('/content/ckpts/CAIN/pretrained_cain.pth')\n",
        "cain.load_state_dict(cain_ckpt['state_dict'])\n",
        "print('Loaded pretrained CAIN')\n",
        "# Load ResNet checkpoints\n",
        "state_dict = torch.load('/content/ckpts/ResNet/resnet_5.pth')\n",
        "resnet.load_state_dict(state_dict['model'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3p5f5byIF9o"
      },
      "outputs": [],
      "source": [
        "input = \"/content/DeepStab/unstable/1.avi\"\n",
        "ouput = \"/content/stable_result.avi\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWtixqM7H421",
        "outputId": "de72995c-a250-44c1-fab0-67bdab822a39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "91422720\n",
            "OK\n",
            "0\n",
            "1\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "stabilize(input, ouput)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g26Mor7oFoQH"
      },
      "source": [
        "tải file output về máy để xem do không thể xem trên cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXe1_XjLFx4V"
      },
      "source": [
        "###2. Thử nghiệm train + test model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7up8XqCd44f9"
      },
      "source": [
        "###CONVERT VIDEO DATA TO IMAGE DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pbXd-ZiD4UC"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def video_to_images(video_path, output_dir):\n",
        "    \"\"\"Converts a video to individual frames and saves them in the output directory.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the video file.\n",
        "        output_dir (str): Path to the directory where frames will be saved.\n",
        "    \"\"\"\n",
        "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    output_video_dir = os.path.join(output_dir, video_name)\n",
        "    os.makedirs(output_video_dir, exist_ok=True)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_filename = os.path.join(output_video_dir, f\"frame{frame_count:04d}.jpg\")\n",
        "        cv2.imwrite(frame_filename, frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "\n",
        "dataset_path = \"/content/DeepStab/unstable\"  # Replace with your dataset path\n",
        "output_dir = \"/content/DeepStab/unstable_frames\"  # Replace with your desired output directory\n",
        "\n",
        "for video_file in os.listdir(dataset_path):\n",
        "    if video_file.endswith(('.mp4', '.avi')):  # Adjust video extensions as needed\n",
        "        video_path = os.path.join(dataset_path, video_file)\n",
        "        video_to_images(video_path, output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vOsh6x-98iF"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/DeepStab/stable\"  # Replace with your dataset path\n",
        "output_dir = \"/content/DeepStab/stable_frames\"  # Replace with your desired output directory\n",
        "\n",
        "for video_file in os.listdir(dataset_path):\n",
        "    if video_file.endswith(('.mp4', '.avi')):  # Adjust video extensions as needed\n",
        "        video_path = os.path.join(dataset_path, video_file)\n",
        "        video_to_images(video_path, output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQpm7UA44zHV"
      },
      "source": [
        "###MAKE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzdh2HVgJlTM"
      },
      "outputs": [],
      "source": [
        "### python lib\n",
        "import os, sys, math, random, glob, cv2, argparse, natsort\n",
        "import numpy as np\n",
        "\n",
        "### torch lib\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as tv\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def read_img(filename, resize= False, scale_factor= 1.0):\n",
        "\n",
        "    ## read image and convert to RGB in [0, 1]\n",
        "\n",
        "    img = cv2.imread(filename)\n",
        "    if resize:\n",
        "        img = cv2.resize(img, (0,0), fx= scale_factor, fy= scale_factor)\n",
        "        #print(img.shape)\n",
        "\n",
        "    if img is None:\n",
        "        raise Exception(\"Image %s does not exist\" %filename)\n",
        "\n",
        "    #img = img[:, :, ::-1] ## BGR to RGB\n",
        "\n",
        "    img = np.float32(img) / 255.0\n",
        "\n",
        "    return img\n",
        "\n",
        "crop_thingies = (0, 0, 0, 0)\n",
        "\n",
        "class RandomCrop(object):\n",
        "    def __init__(self, image_size, crop_size):\n",
        "        self.ch, self.cw = crop_size\n",
        "        ih, iw = image_size\n",
        "\n",
        "        self.h1 = random.randint(0, ih - self.ch - 0)\n",
        "        self.w1 = random.randint(0, iw - self.cw - 0)\n",
        "\n",
        "        self.h2 = self.h1 + self.ch\n",
        "        self.w2 = self.w1 + self.cw\n",
        "        global crop_thingies\n",
        "        crop_thingies = (self.h1, self.h2, self.w1, self.w2)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        #print(\"Came in to cropper...\", crop_thingies)\n",
        "        #sys.exit()\n",
        "        if len(img.shape) == 3:\n",
        "            return img[self.h1 : self.h2, self.w1 : self.w2, :]\n",
        "        else:\n",
        "            return img[self.h1 : self.h2, self.w1 : self.w2]\n",
        "\n",
        "\n",
        "class FixedCrop(object):\n",
        "    def __init__(self, crop_size, mh, mw):\n",
        "        self.ch, self.cw = crop_size\n",
        "        self.h1 = mh\n",
        "        self.w1 = mw\n",
        "        self.h2 = self.h1 + self.ch\n",
        "        self.w2 = self.w1 + self.cw\n",
        "        #global crop_thingies\n",
        "        #crop_thingies = (self.h1, self.h2, self.w1, self.w2)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        #print(\"Came in to cropper...\", crop_thingies)\n",
        "        #sys.exit()\n",
        "        if len(img.shape) == 3:\n",
        "            return img[self.h1 : self.h2, self.w1 : self.w2, :]\n",
        "        else:\n",
        "            return img[self.h1 : self.h2, self.w1 : self.w2]\n",
        "\n",
        "\n",
        "\n",
        "class MultiFramesDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, mode= 'train', opts= None, get_lbls= False, prev_iter_counter= 0):\n",
        "        super(MultiFramesDataset, self).__init__()\n",
        "        self.mode = mode\n",
        "        self.ip_dir = opts.data_dir\n",
        "        self.dataset_task_list = []\n",
        "        self.num_frames = []\n",
        "        self.get_lbls = get_lbls\n",
        "        self.iter_counter = prev_iter_counter\n",
        "        if self.mode == \"train\":\n",
        "            self.sample_frames = opts.sample_frames\n",
        "            self.geometry_aug = opts.geometry_aug\n",
        "            self.scale_min = opts.scale_min\n",
        "            self.scale_max = opts.scale_max\n",
        "            self.crop_size = opts.crop_size\n",
        "            self.order_aug = opts.order_aug\n",
        "            self.size_multiplier = opts.size_multiplier\n",
        "\n",
        "            self.dataset_task_list = natsort.natsorted(glob.glob(os.path.join(self.ip_dir + \"*/\")))\n",
        "            self.dataset_lbls_list = natsort.natsorted(glob.glob(os.path.join(opts.train_lbl_dir + \"*/\")))\n",
        "            self.shuffled_inds = np.arange(len(self.dataset_task_list))\n",
        "            np.random.shuffle(self.shuffled_inds)\n",
        "            self.num_frames = [len(os.listdir(dir)) for dir in self.dataset_task_list]\n",
        "            #list_lbl = natsort.natsorted(glob.glob(os.path.join(self.lbl_dir + \"*/\")))\n",
        "        elif self.mode == \"val\":\n",
        "            self.dataset_task_list = natsort.natsorted(glob.glob(os.path.join(self.ip_dir + \"*/\")))\n",
        "            #self.dataset_task_list = [os.path.join(self.ip_dir+i+'/') for i in ['Crowd_7','Parallax_10','Regular_16','Running_16','Zooming_11']]\n",
        "            self.num_frames = [len(os.listdir(dir)) for dir in self.dataset_task_list]\n",
        "            # print(self.dataset_task_list)\n",
        "        else:\n",
        "            raise ValueError(\"Only train and val mode is implemented at the moment...\")\n",
        "\n",
        "        print(\"+\"*20, \"Total task_list:\", \"+\"*20)\n",
        "        video_names = []\n",
        "        for task in self.dataset_task_list:\n",
        "            video_names.append(task.split(\"/\")[-2])\n",
        "        print(video_names)\n",
        "        print(\"+\"*50)\n",
        "        self.num_tasks = len(self.dataset_task_list)\n",
        "\n",
        "\n",
        "        global crop_thingies\n",
        "\n",
        "        print(\"[%s] Total %d videos (%d frames)\" %(self.__class__.__name__, len(self.dataset_task_list), sum(self.num_frames)))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_task_list)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        meta_data = {}\n",
        "        meta_data['idx'] = index\n",
        "        ## random select starting frame index t between [0, N - number_of_sample_frames] for \"mode = \"train\" | \"validate\"\"\n",
        "        if self.mode == \"train\":\n",
        "            index_ = self.shuffled_inds[index]\n",
        "            N = self.num_frames[index_]\n",
        "            T = random.randint(0, N - self.sample_frames)\n",
        "            #T = 0 #### Change this again for later experiments to randomize the frames for training\n",
        "            meta_data['starting_frame'] = T\n",
        "\n",
        "            input_dir = self.dataset_task_list[index_]\n",
        "            if self.get_lbls:\n",
        "                lbl_dir = self.dataset_lbls_list[index_]\n",
        "            meta_data['unstable_video_path'] = input_dir\n",
        "            if self.get_lbls:\n",
        "                meta_data['stable_video_path'] = lbl_dir\n",
        "\n",
        "            ## sample from T to T + #sample_frames - 1\n",
        "            frame_ip = []\n",
        "            if self.get_lbls:\n",
        "                frame_lbl = []\n",
        "            ip_frame_list = natsort.natsorted(glob.glob(os.path.join(input_dir, \"*.*\")))\n",
        "            if self.get_lbls:\n",
        "                lbl_frame_list = natsort.natsorted(glob.glob(os.path.join(lbl_dir, \"*.*\")))\n",
        "            for t in range(T, T + self.sample_frames):\n",
        "                if self.iter_counter < 200000:\n",
        "                    frame_ip.append(read_img(ip_frame_list[t], True, 1.0))\n",
        "                    if self.get_lbls:\n",
        "                        frame_lbl.append(read_img(lbl_frame_list[t], True, 1.0))\n",
        "                else:\n",
        "                    frame_ip.append(read_img(ip_frame_list[t], False, 1.0))\n",
        "                    if self.get_lbls:\n",
        "                        frame_lbl.append(read_img(lbl_frame_list[t], False, 1.0))\n",
        "\n",
        "            meta_data['ip_frame_paths'] = ip_frame_list[T:T + self.sample_frames]\n",
        "            if self.get_lbls:\n",
        "                meta_data['op_frame_paths'] = lbl_frame_list[T:T + self.sample_frames]\n",
        "\n",
        "            self.iter_counter += 1\n",
        "\n",
        "            ## data augmentation\n",
        "            if self.geometry_aug:\n",
        "\n",
        "                ## random scale\n",
        "                H_in = frame_ip[0].shape[0]\n",
        "                W_in = frame_ip[0].shape[1]\n",
        "\n",
        "                sc = np.random.uniform(self.scale_min, self.scale_max)\n",
        "                H_out = int(math.floor(H_in * sc))\n",
        "                W_out = int(math.floor(W_in * sc))\n",
        "\n",
        "                ## scaled size should be equal to opts.crop_size\n",
        "                if H_out < W_out:\n",
        "                    if H_out < self.crop_size:\n",
        "                        H_out = self.crop_size\n",
        "                        W_out = int(math.floor(W_in * float(H_out) / float(H_in)))\n",
        "                else: ## W_out < H_out\n",
        "                    if W_out < self.crop_size:\n",
        "                        W_out = self.crop_size\n",
        "                        H_out = int(math.floor(H_in * float(W_out) / float(W_in)))\n",
        "\n",
        "                for t in range(self.sample_frames):\n",
        "                    frame_ip[t] = cv2.resize(frame_ip[t], (W_out, H_out))\n",
        "                    if self.get_lbls:\n",
        "                        frame_lbl[t] = cv2.resize(frame_lbl[t], (W_out, H_out))\n",
        "                meta_data['scale_factor'] = sc\n",
        "\n",
        "            ## random crop\n",
        "            cropper = RandomCrop(frame_ip[0].shape[:2], (self.crop_size, self.crop_size))\n",
        "\n",
        "\n",
        "            for t in range(self.sample_frames):\n",
        "                frame_ip[t] = cropper(frame_ip[t])\n",
        "                if self.get_lbls:\n",
        "                    frame_lbl[t] = cropper(frame_lbl[t])\n",
        "                if frame_ip[t].shape[1] != self.crop_size and frame_ip[t].shape[0] != self.crop_size:\n",
        "                    print(\"[MultiFrameDataset]: size mismatch occured... =>\", frame_ip[t].shape)\n",
        "                if self.get_lbls:\n",
        "                        if frame_lbl[t].shape[1] != self.crop_size and frame_lbl[t].shape[0] != self.crop_size:\n",
        "                            print(\"[MultiFrameDataset]: size mismatch occured... =>\", frame_lbl[t].shape)\n",
        "            meta_data['crop_coords'] = crop_thingies\n",
        "\n",
        "\n",
        "            if self.geometry_aug:\n",
        "                #meta_data['rotation'] = False\n",
        "                ### random rotate\n",
        "                rotate = random.randint(0, 3)\n",
        "                if rotate != 0:\n",
        "                    for t in range(self.sample_frames):\n",
        "                        frame_ip[t] = np.rot90(frame_ip[t], rotate)\n",
        "                        if self.get_lbls:\n",
        "                            frame_lbl[t] = np.rot90(frame_lbl[t], rotate)\n",
        "                    #meta_data['rotation'] = True\n",
        "\n",
        "                ## horizontal flip\n",
        "                if np.random.random() >= 0.5:\n",
        "                    for t in range(self.sample_frames):\n",
        "                        frame_ip[t] = cv2.flip(frame_ip[t], flipCode=0)\n",
        "                        if self.get_lbls:\n",
        "                            frame_lbl[t] = cv2.flip(frame_lbl[t], flipCode=0)\n",
        "                    #meta_data['hflip'] = True\n",
        "\n",
        "\n",
        "            if self.order_aug:\n",
        "                ## reverse temporal order\n",
        "                #meta_data['order'] = \"normal\"\n",
        "                if np.random.random() >= 0.5:\n",
        "                    #meta_data['order'] = \"reversed\"\n",
        "                    frame_ip.reverse()\n",
        "                    if self.get_lbls:\n",
        "                        frame_lbl.reverse()\n",
        "\n",
        "        elif self.mode == \"val\":\n",
        "            input_dir = self.dataset_task_list[index]\n",
        "            #lbl_dir = self.dataset_task_list[index][1]\n",
        "            meta_data['unstable_video_path'] = input_dir\n",
        "            meta_data['video_name'] = input_dir\n",
        "            #meta_data['stable_video_path'] = lbl_dir\n",
        "\n",
        "            ## sample from T to T + #sample_frames - 1\n",
        "            frame_ip = []\n",
        "            #frame_lbl = []\n",
        "            ip_frame_list = natsort.natsorted(glob.glob(os.path.join(input_dir, \"*.*\")))\n",
        "            #lbl_frame_list = natsort.natsorted(glob.glob(os.path.join(lbl_dir, \"*.*\")))\n",
        "            for t in range(0, self.num_frames[index]):\n",
        "                frame_ip.append(read_img(ip_frame_list[t]))\n",
        "                #frame_lbl.append(read_img(lbl_frame_list[t]))\n",
        "            meta_data['ip_frame_paths'] = ip_frame_list\n",
        "\n",
        "        ### convert (H, W, C) array to (C, H, W) tensor\n",
        "        X = []\n",
        "        if self.mode == \"train\":\n",
        "            if self.get_lbls:\n",
        "                Y = []\n",
        "            for t in range(len(frame_ip)):\n",
        "                X.append(torch.from_numpy(frame_ip[t].transpose(2, 0, 1).astype(np.float32)))\n",
        "                if self.get_lbls:\n",
        "                    Y.append(torch.from_numpy(frame_lbl[t].transpose(2, 0, 1).astype(np.float32)))\n",
        "\n",
        "            if self.get_lbls:\n",
        "                return {'X': X, 'Y': Y, 'meta_data': meta_data}\n",
        "            else:\n",
        "                return {'X': X, 'meta_data': meta_data}\n",
        "\n",
        "        else:\n",
        "            for t in range(len(frame_ip)):\n",
        "                X.append(torch.unsqueeze(torch.from_numpy(frame_ip[t].transpose(2, 0, 1).astype(np.float32)), 0))\n",
        "\n",
        "            return {'X': X, 'meta_data': meta_data}\n",
        "\n",
        "\n",
        "class MultiFramesDatasetHybrid(data.Dataset):\n",
        "\n",
        "    def __init__(self, mode= 'train', opts= None, get_lbls= True):\n",
        "        super(MultiFramesDatasetHybrid, self).__init__()\n",
        "        self.ip_dir = opts.data_dir\n",
        "        self.ip_dir_synth = opts.data_dir_synth\n",
        "        self.lbl_dir = opts.train_lbl_dir\n",
        "        self.sample_frames = opts.sample_frames\n",
        "        self.geometry_aug = opts.geometry_aug\n",
        "        self.scale_min = opts.scale_min\n",
        "        self.scale_max = opts.scale_max\n",
        "        self.crop_size = opts.crop_size\n",
        "        self.order_aug = opts.order_aug\n",
        "        self.size_multiplier = opts.size_multiplier\n",
        "        self.mode = mode\n",
        "        self.dataset_task_list = []\n",
        "        self.num_frames = []\n",
        "        self.get_lbls = get_lbls\n",
        "        self.iter_counter = 0\n",
        "        if self.mode == \"train\":\n",
        "            self.dataset_task_list = natsort.natsorted(glob.glob(os.path.join(self.ip_dir + \"*/\")))\n",
        "            self.dataset_syn_ip_list = natsort.natsorted(glob.glob(os.path.join(self.ip_dir_synth + \"*/\")))\n",
        "            self.dataset_syn_lbls_list = natsort.natsorted(glob.glob(os.path.join(self.lbl_dir + \"*/\")))\n",
        "            self.shuffled_inds = np.arange(len(self.dataset_task_list))\n",
        "            np.random.shuffle(self.shuffled_inds)\n",
        "            #list_lbl = natsort.natsorted(glob.glob(os.path.join(self.lbl_dir + \"*/\")))\n",
        "        elif self.mode == \"val\":\n",
        "            self.dataset_task_list = natsort.natsorted(glob.glob(os.path.join(self.ip_dir + \"*/\")))\n",
        "        else:\n",
        "            raise ValueError(\"Only train and val mode is implemented at the moment...\")\n",
        "\n",
        "        print(\"+\"*20, \"Total task_list:\", \"+\"*20)\n",
        "        video_names = []\n",
        "        for task in self.dataset_task_list:\n",
        "            video_names.append(task.split(\"/\")[-2])\n",
        "        print(video_names)\n",
        "        print(\"+\"*50)\n",
        "        self.num_tasks = len(self.dataset_task_list)\n",
        "\n",
        "        for ip in self.dataset_task_list:\n",
        "            self.num_frames.append(len(natsort.natsorted(os.listdir(ip))))\n",
        "\n",
        "        for ip in self.synth_list:\n",
        "            self.num_frames_synth.append(len(natsort.natsorted(os.listdir(ip))))\n",
        "\n",
        "        global crop_thingies\n",
        "\n",
        "        print(\"[%s] Total %d videos (%d frames)\" %(self.__class__.__name__, len(self.dataset_task_list), sum(self.num_frames)))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_task_list)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        meta_data = {}\n",
        "        meta_data['idx'] = index\n",
        "        ## random select starting frame index t between [0, N - number_of_sample_frames] for \"mode = \"train\" | \"validate\"\"\n",
        "        if self.mode == \"train\":\n",
        "            index_ = self.shuffled_inds[index]\n",
        "            rand_num = random.randint(0, len(self.synth_list))\n",
        "            N = self.num_frames[index_]\n",
        "            Ns = self.num_frames_synth[rand_num]\n",
        "            T = random.randint(0, N - self.sample_frames)\n",
        "            Ts = random.randint(0, Ns - self.sample_frames)\n",
        "            #T = 0 #### Change this again for later experiments to randomize the frames for training\n",
        "            meta_data['starting_frame'] = T\n",
        "\n",
        "            input_dir = self.dataset_task_list[index_]\n",
        "            if self.get_lbls:\n",
        "                syn_ip_dir = self.dataset_syn_ip_list[rand_num]\n",
        "                lbl_dir = self.dataset_syn_lbls_list[rand_num]\n",
        "            meta_data['unstable_video_path'] = input_dir\n",
        "            if self.get_lbls:\n",
        "                meta_data['stable_video_path'] = lbl_dir\n",
        "\n",
        "            ## sample from T to T + #sample_frames - 1\n",
        "            frame_ip = []\n",
        "            if self.get_lbls:\n",
        "                frame_syn = []\n",
        "                frame_lbl = []\n",
        "            ip_frame_list = natsort.natsorted(glob.glob(os.path.join(input_dir, \"*.*\")))\n",
        "            if self.get_lbls:\n",
        "                syn_ip_list = natsort.natsorted(glob.glob(os.path.join(syn_ip_dir, \"*.*\")))\n",
        "                lbl_frame_list = natsort.natsorted(glob.glob(os.path.join(lbl_dir, \"*.*\")))\n",
        "\n",
        "            for t in range(T, T + self.sample_frames):\n",
        "                if self.iter_counter < 1000:\n",
        "                    frame_ip.append(read_img(ip_frame_list[t], True, 0.3))\n",
        "                    if self.get_lbls:\n",
        "                        frame_syn.append(read_img(syn_ip_list[t], True, 0.3))\n",
        "                        frame_lbl.append(read_img(lbl_frame_list[t], True, 0.3))\n",
        "                elif self.iter_counter < 2000:\n",
        "                    frame_ip.append(read_img(ip_frame_list[t], True, 0.4))\n",
        "                    if self.get_lbls:\n",
        "                        frame_syn.append(read_img(syn_ip_list[t], True, 0.4))\n",
        "                        frame_lbl.append(read_img(lbl_frame_list[t], True, 0.4))\n",
        "                elif self.iter_counter < 3000:\n",
        "                    frame_ip.append(read_img(ip_frame_list[t], True, 0.45))\n",
        "                    if self.get_lbls:\n",
        "                        frame_syn.append(read_img(syn_ip_list[t], True, 0.45))\n",
        "                        frame_lbl.append(read_img(lbl_frame_list[t], True, 0.45))\n",
        "                elif self.iter_counter < 4000:\n",
        "                    frame_ip.append(read_img(ip_frame_list[t], True, 0.5))\n",
        "                    if self.get_lbls:\n",
        "                        frame_syn.append(read_img(syn_ip_list[t], True, 0.5))\n",
        "                        frame_lbl.append(read_img(lbl_frame_list[t], True, 0.5))\n",
        "                elif self.iter_counter < 5000:\n",
        "                    frame_ip.append(read_img(ip_frame_list[t], True, 0.55))\n",
        "                    if self.get_lbls:\n",
        "                        frame_syn.append(read_img(syn_ip_list[t], True, 0.55))\n",
        "                        frame_lbl.append(read_img(lbl_frame_list[t], True, 0.55))\n",
        "                elif self.iter_counter < 6000:\n",
        "                    frame_ip.append(read_img(ip_frame_list[t], True, 0.6))\n",
        "                    if self.get_lbls:\n",
        "                        frame_syn.append(read_img(syn_ip_list[t], True, 0.6))\n",
        "                        frame_lbl.append(read_img(lbl_frame_list[t], True, 0.6))\n",
        "                elif self.iter_counter < 7000:\n",
        "                    frame_ip.append(read_img(ip_frame_list[t], True, 0.7))\n",
        "                    if self.get_lbls:\n",
        "                        frame_syn.append(read_img(syn_ip_list[t], True, 0.7))\n",
        "                        frame_lbl.append(read_img(lbl_frame_list[t], True, 0.7))\n",
        "                elif self.iter_counter < 8000:\n",
        "                    frame_ip.append(read_img(ip_frame_list[t], True, 0.8))\n",
        "                    if self.get_lbls:\n",
        "                        frame_syn.append(read_img(syn_ip_list[t], True, 0.8))\n",
        "                        frame_lbl.append(read_img(lbl_frame_list[t], True, 0.8))\n",
        "                elif self.iter_counter < 9000:\n",
        "                    frame_ip.append(read_img(ip_frame_list[t], True, 0.9))\n",
        "                    if self.get_lbls:\n",
        "                        frame_syn.append(read_img(syn_ip_list[t], True, 0.9))\n",
        "                        frame_lbl.append(read_img(lbl_frame_list[t], True, 0.9))\n",
        "                else:\n",
        "                    frame_ip.append(read_img(ip_frame_list[t], False, 1.0))\n",
        "                    if self.get_lbls:\n",
        "                        frame_syn.append(read_img(syn_ip_list[t], True, 1.0))\n",
        "                        frame_lbl.append(read_img(lbl_frame_list[t], False, 1.0))\n",
        "\n",
        "            meta_data['ip_frame_paths'] = ip_frame_list[T:T + self.sample_frames]\n",
        "            if self.get_lbls:\n",
        "                meta_data['op_frame_paths'] = lbl_frame_list[T:T + self.sample_frames]\n",
        "\n",
        "            self.iter_counter += 1\n",
        "\n",
        "            ## data augmentation\n",
        "            if self.geometry_aug:\n",
        "\n",
        "                ## random scale\n",
        "                H_in = frame_ip[0].shape[0]\n",
        "                W_in = frame_ip[0].shape[1]\n",
        "\n",
        "                sc = np.random.uniform(self.scale_min, self.scale_max)\n",
        "                H_out = int(math.floor(H_in * sc))\n",
        "                W_out = int(math.floor(W_in * sc))\n",
        "\n",
        "                ## scaled size should be equal to opts.crop_size\n",
        "                if H_out < W_out:\n",
        "                    if H_out < self.crop_size:\n",
        "                        H_out = self.crop_size\n",
        "                        W_out = int(math.floor(W_in * float(H_out) / float(H_in)))\n",
        "                else: ## W_out < H_out\n",
        "                    if W_out < self.crop_size:\n",
        "                        W_out = self.crop_size\n",
        "                        H_out = int(math.floor(H_in * float(W_out) / float(W_in)))\n",
        "\n",
        "                for t in range(self.sample_frames):\n",
        "                    frame_ip[t] = cv2.resize(frame_ip[t], (W_out, H_out))\n",
        "                    if self.get_lbls:\n",
        "                        frame_syn[t] = cv2.resize(frame_syn[t], (W_out, H_out))\n",
        "                        frame_lbl[t] = cv2.resize(frame_lbl[t], (W_out, H_out))\n",
        "\n",
        "                meta_data['scale_factor'] = sc\n",
        "\n",
        "            ## random crop\n",
        "            cropper = RandomCrop(frame_ip[0].shape[:2], (self.crop_size, self.crop_size))\n",
        "\n",
        "\n",
        "            for t in range(self.sample_frames):\n",
        "                frame_ip[t] = cropper(frame_ip[t])\n",
        "                if self.get_lbls:\n",
        "                    frame_lbl[t] = cropper(frame_lbl[t])\n",
        "                    frame_syn[t] = cropper(frame_syn[t])\n",
        "                if frame_ip[t].shape[1] != self.crop_size and frame_ip[t].shape[0] != self.crop_size:\n",
        "                    print(\"[MultiFrameDataset]: size mismatch occured... =>\", frame_ip[t].shape)\n",
        "                if self.get_lbls:\n",
        "                        if frame_lbl[t].shape[1] != self.crop_size and frame_lbl[t].shape[0] != self.crop_size:\n",
        "                            print(\"[MultiFrameDataset]: size mismatch occured... =>\", frame_lbl[t].shape)\n",
        "                        if frame_syn[t].shape[1] != self.crop_size and frame_syn[t].shape[0] != self.crop_size:\n",
        "                            print(\"[MultiFrameDataset]: size mismatch occured... =>\", frame_syn[t].shape)\n",
        "            meta_data['crop_coords'] = crop_thingies\n",
        "\n",
        "\n",
        "            if self.geometry_aug:\n",
        "                #meta_data['rotation'] = False\n",
        "                ### random rotate\n",
        "                rotate = random.randint(0, 3)\n",
        "                if rotate != 0:\n",
        "                    for t in range(self.sample_frames):\n",
        "                        frame_ip[t] = np.rot90(frame_ip[t], rotate)\n",
        "                        if self.get_lbls:\n",
        "                            frame_syn[t] = np.rot90(frame_syn[t], rotate)\n",
        "                            frame_lbl[t] = np.rot90(frame_lbl[t], rotate)\n",
        "                    #meta_data['rotation'] = True\n",
        "\n",
        "                ## horizontal flip\n",
        "                if np.random.random() >= 0.5:\n",
        "                    for t in range(self.sample_frames):\n",
        "                        frame_ip[t] = cv2.flip(frame_ip[t], flipCode=0)\n",
        "                        if self.get_lbls:\n",
        "                            frame_syn[t] = cv2.flip(frame_syn[t], flipCode=0)\n",
        "                            frame_lbl[t] = cv2.flip(frame_lbl[t], flipCode=0)\n",
        "                    #meta_data['hflip'] = True\n",
        "\n",
        "\n",
        "            if self.order_aug:\n",
        "                ## reverse temporal order\n",
        "                #meta_data['order'] = \"normal\"\n",
        "                if np.random.random() >= 0.5:\n",
        "                    #meta_data['order'] = \"reversed\"\n",
        "                    frame_ip.reverse()\n",
        "                    if self.get_lbls:\n",
        "                        frame_syn.reverse()\n",
        "                        frame_lbl.reverse()\n",
        "\n",
        "        elif self.mode == \"val\":\n",
        "            input_dir = self.dataset_task_list[index]\n",
        "            #lbl_dir = self.dataset_task_list[index][1]\n",
        "            meta_data['unstable_video_path'] = input_dir\n",
        "            meta_data['video_name'] = input_dir\n",
        "            #meta_data['stable_video_path'] = lbl_dir\n",
        "\n",
        "            ## sample from T to T + #sample_frames - 1\n",
        "            frame_ip = []\n",
        "            #frame_lbl = []\n",
        "            ip_frame_list = natsort.natsorted(glob.glob(os.path.join(input_dir, \"*.*\")))\n",
        "            #lbl_frame_list = natsort.natsorted(glob.glob(os.path.join(lbl_dir, \"*.*\")))\n",
        "            for t in range(0, self.sample_frames):\n",
        "                frame_ip.append(read_img(ip_frame_list[t]))\n",
        "                #frame_lbl.append(read_img(lbl_frame_list[t]))\n",
        "            meta_data['ip_frame_paths'] = ip_frame_list\n",
        "\n",
        "        ### convert (H, W, C) array to (C, H, W) tensor\n",
        "        X = []\n",
        "        if self.mode == \"train\":\n",
        "            if self.get_lbls:\n",
        "                X2 = []\n",
        "                Y = []\n",
        "            for t in range(len(frame_ip)):\n",
        "                X.append(torch.from_numpy(frame_ip[t].transpose(2, 0, 1).astype(np.float32)))\n",
        "                if self.get_lbls:\n",
        "                    X2.append(torch.from_numpy(frame_syn[t].transpose(2, 0, 1).astype(np.float32)))\n",
        "                    Y.append(torch.from_numpy(frame_lbl[t].transpose(2, 0, 1).astype(np.float32)))\n",
        "\n",
        "\n",
        "            if self.get_lbls:\n",
        "                return {'X': X, 'X2': X2, 'Y': Y, 'meta_data': meta_data}\n",
        "            else:\n",
        "                return {'X': X, 'meta_data': meta_data}\n",
        "\n",
        "        else:\n",
        "            for t in range(len(frame_ip)):\n",
        "                X.append(torch.unsqueeze(torch.from_numpy(frame_ip[t].transpose(2, 0, 1).astype(np.float32)), 0))\n",
        "\n",
        "            return {'X': X, 'meta_data': meta_data}\n",
        "\n",
        "class SubsetSequentialSampler(Sampler):\n",
        "\n",
        "    def __init__(self, indices):\n",
        "        self.indices = indices\n",
        "\n",
        "    def __iter__(self):\n",
        "        return (self.indices[i] for i in range(len(self.indices)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "def create_data_loader(data_set, mode, train_epoch_size= 1000, batch_size= 4, threads= 8):\n",
        "\n",
        "    ### generate random index\n",
        "    if mode == 'train':\n",
        "        total_samples = train_epoch_size * batch_size\n",
        "    else:\n",
        "        raise ValueError(\"Only train mode is implemented at the moment...\")\n",
        "\n",
        "    num_epochs = int(math.ceil(float(total_samples) / len(data_set)))\n",
        "\n",
        "    indices = np.random.permutation(len(data_set))\n",
        "    indices = np.tile(indices, num_epochs)\n",
        "    indices = indices[:total_samples]\n",
        "\n",
        "    ### generate data sampler and loader\n",
        "    sampler = SubsetSequentialSampler(indices)\n",
        "    if mode == \"train\":\n",
        "        data_loader = DataLoader(dataset= data_set, num_workers= threads, batch_size= batch_size, sampler= sampler, pin_memory= True)\n",
        "\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfESiJNP3wEm"
      },
      "source": [
        "###LOSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "JcSd_DZA3tR_",
        "outputId": "0efd884f-883f-4df5-d82c-b0e1a429fab8"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'networks'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2c059ab80b92>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontextual_loss\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_cs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoarseStabilizerInferReady\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'networks'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import cv2\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import contextual_loss as cx\n",
        "# from model_cs import CoarseStabilizerInferReady\n",
        "\n",
        "\n",
        "# def gradient_loss(gen_frames, gt_frames, alpha=1):\n",
        "\n",
        "#     def gradient(x):\n",
        "#         # idea from tf.image.image_gradients(image)\n",
        "#         # https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/image_ops_impl.py#L3441-L3512\n",
        "#         # x: (b,c,h,w), float32 or float64\n",
        "#         # dx, dy: (b,c,h,w)\n",
        "\n",
        "#         h_x = x.size()[-2]\n",
        "#         w_x = x.size()[-1]\n",
        "#         # gradient step=1\n",
        "#         left = x\n",
        "#         right = F.pad(x, [0, 1, 0, 0])[:, :, :, 1:]\n",
        "#         top = x\n",
        "#         bottom = F.pad(x, [0, 0, 0, 1])[:, :, 1:, :]\n",
        "\n",
        "#         # dx, dy = torch.abs(right - left), torch.abs(bottom - top)\n",
        "#         dx, dy = right - left, bottom - top\n",
        "#         # dx will always have zeros in the last column, right-left\n",
        "#         # dy will always have zeros in the last row,    bottom-top\n",
        "#         dx[:, :, :, -1] = 0\n",
        "#         dy[:, :, -1, :] = 0\n",
        "\n",
        "#         return dx, dy\n",
        "\n",
        "#     # gradient\n",
        "#     gen_dx, gen_dy = gradient(gen_frames)\n",
        "#     gt_dx, gt_dy = gradient(gt_frames)\n",
        "#     #\n",
        "#     grad_diff_x = torch.abs(gt_dx - gen_dx)\n",
        "#     grad_diff_y = torch.abs(gt_dy - gen_dy)\n",
        "\n",
        "#     # condense into one tensor and avg\n",
        "#     return torch.mean(grad_diff_x ** alpha + grad_diff_y ** alpha)\n",
        "\n",
        "\n",
        "# def gram(x):\n",
        "# \tb,c,h,w = x.size();\n",
        "# \tx = x.view(b*c, -1);\n",
        "# \treturn torch.mm(x, x.t())\n",
        "\n",
        "\n",
        "# class outer_loop_loss_affine(nn.Module):\n",
        "# \tdef __init__(self, flownet, interpolator, summary_writer= None):\n",
        "# \t\tsuper(outer_loop_loss_affine, self).__init__()\n",
        "# \t\tself.crit = cx.ContextualLoss(use_vgg= True, vgg_layer='relu3_4', band_width= 0.1, loss_type= 'cosine').cuda()\n",
        "# \t\tself.flownet = flownet\n",
        "# \t\tself.summary_writer = summary_writer\n",
        "# \t\tself.writer_counter = 0\n",
        "# \t\tself.interpolator = interpolator\n",
        "# \t\tself.l2 = nn.MSELoss(size_average=True)\n",
        "# \t\tself.l1 = nn.L1Loss()\n",
        "# \t\t#self.affine = CoarseStabilizerInferReady(self.flownet)\n",
        "\n",
        "\n",
        "# \tdef forward(self, semi_stable, stable):\n",
        "# \t\t#### Flow loss: \tloss between semi stable and transformed (Working: minimize the motion (flow) between the generated and transformed (coarsely stable) frames)...\n",
        "# \t\t#### Pixel loss:\tpixel space loss between the generated and transformed stable...\n",
        "\n",
        "# \t\tmotion_between_gen_and_transformed = 0\n",
        "# \t\tpixel_loss = 0\n",
        "\n",
        "\n",
        "# \t\tfor i in range(1, len(semi_stable)):\n",
        "# \t\t\t#### Flow loss\n",
        "# \t\t\tflow_stable = self.flownet.estimateFlowFull(stable[i - 1], stable[i])\n",
        "# \t\t\tflow_unstable = self.flownet.estimateFlowFull(semi_stable[i - 1], semi_stable[i])\n",
        "# \t\t\tmotion_between_gen_and_transformed = motion_between_gen_and_transformed + self.l2(flow_unstable, flow_stable)\n",
        "\n",
        "\n",
        "# \t\t\t#motion_between_gen_and_transformed = motion_between_gen_and_transformed + self.l2(torch.zeros_like(flow).cuda(), flow)\n",
        "\n",
        "# \t\t\t#### Pixel loss\n",
        "# \t\t\tpixel_loss += self.crit(semi_stable[i], stable[i])\n",
        "\n",
        "# \t\tcurr_loss = (10.0*torch.abs(motion_between_gen_and_transformed) + 100.0*pixel_loss)/(len(semi_stable) )\n",
        "\n",
        "\n",
        "\n",
        "# \t\tif self.summary_writer is not None:\n",
        "# \t\t\tself.summary_writer.add_scalar('outer_loop_loss/pixel_loss', pixel_loss.item(), self.writer_counter)\n",
        "# \t\t\tself.summary_writer.add_scalar('outer_loop_loss/motion_loss', torch.abs(motion_between_gen_and_transformed).item(), self.writer_counter)\n",
        "# \t\t\tself.writer_counter += 1\n",
        "\n",
        "# \t\treturn curr_loss\n",
        "\n",
        "# \tdef epe(self, tenFlow, tenTruth):\n",
        "# \t\ttenEpe = torch.mean(torch.sqrt((tenFlow[:, 0:1, :, :] - tenTruth[:, 0:1, :, :])**2 + (tenFlow[:, 1:2, :, :] - tenTruth[:, 1:2, :, :])**2))\n",
        "# \t\treturn tenEpe\n",
        "\n",
        "\n",
        "# class inner_loop_loss_perc_affine_another_try(nn.Module):\n",
        "# \tdef __init__(self, loss_fn, interpolator, flownet, summary_writer= None, use_vgg= True):\n",
        "# \t\tsuper(inner_loop_loss_perc_affine_another_try, self).__init__()\n",
        "# \t\tself.summary_writer = summary_writer\n",
        "# \t\tself.writer_counter = 0\n",
        "# \t\tself.crit = loss_fn\n",
        "# \t\tself.cx = cx.ContextualLoss(use_vgg= True, vgg_layer='relu3_4', band_width= 0.1, loss_type= 'cosine').cuda()\n",
        "# \t\tself.interpolator = interpolator\n",
        "# \t\tself.flownet = flownet\n",
        "# \t\tif use_vgg:\n",
        "# \t\t\tself.use_vgg = True\n",
        "# \t\t\tself.norm_vgg = utils.normalize_ImageNet_stats\n",
        "# \t\t\tself.VGG = networks.Vgg16(requires_grad= False)\n",
        "# \t\t\tself.VGG = self.VGG.cuda()\n",
        "# \t\t\tVGGLayers = [int(layer) for layer in list(\"4\")]\n",
        "# \t\t\tVGGLayers.sort()\n",
        "# \t\t\tself.VGGLayers = [layer-1 for layer in list(VGGLayers)]\n",
        "\n",
        "# \t\tself.affine = CoarseStabilizerInferReady(self.flownet)\n",
        "\n",
        "\n",
        "# \tdef forward(self, semi_stable, unstable):\n",
        "# \t\t#with torch.no_grad():\n",
        "# \t\tunstable_txs = []\n",
        "# \t\tunstable_affs = []\n",
        "# \t\tfor i in range(1, len(unstable)):\n",
        "# \t\t\ttx, op = self.affine(unstable[0], unstable[i])\n",
        "# \t\t\tunstable_txs.append(tx)\n",
        "# \t\t\tunstable_affs.append(op)\n",
        "\n",
        "# \t\t#### Flow loss: \tloss between semi stable and transformed (Working: minimize the motion (flow) between the generated and transformed (coarsely stable) frames)...\n",
        "# \t\t#### Affine loss: \tloss between the original and lerped transforms...\n",
        "# \t\t#### Pixel loss:\tpixel space loss between the generated and transformed stable...\n",
        "# \t\tmotion_between_gen_and_transformed = 0\n",
        "# \t\taff_loss = 0\n",
        "# \t\tpixel_loss = 0\n",
        "# \t\tvgg_loss = 0\n",
        "# \t\tfor i in range(1, len(semi_stable)):\n",
        "# \t\t\t#### Flow loss\n",
        "# \t\t\tflow = self.flownet(unstable_txs[i - 1], semi_stable[i])\n",
        "\n",
        "# \t\t\tmotion_between_gen_and_transformed = motion_between_gen_and_transformed + (torch.mean(flow))\n",
        "\n",
        "# \t\t\t#### Pixel loss + Affine loss\n",
        "# \t\t\tpixel_loss = pixel_loss + self.cx(semi_stable[i], unstable[i])\n",
        "\n",
        "# \t\t\tf_x = self.norm_vgg(semi_stable[i])\n",
        "# \t\t\tf_y = self.norm_vgg(unstable_txs[i - 1])\n",
        "# \t\t\tf_yy = self.norm_vgg(unstable[i])\n",
        "# \t\t\tfor l in self.VGGLayers:\n",
        "# \t\t\t\tf_x = self.VGG(f_x, self.VGGLayers[-1])\n",
        "# \t\t\t\tf_y = self.VGG(f_y, self.VGGLayers[-1])\n",
        "# \t\t\t\tf_yy = self.VGG(f_yy, self.VGGLayers[-1])\n",
        "# \t\t\t\tvgg_loss = vgg_loss + self.crit(f_x[l], f_y[l])\n",
        "# \t\t\t\tvgg_loss = vgg_loss + self.crit(gram(f_x[l]), gram(f_yy[l]))\n",
        "# \t\t\t#if i < len(semi_stable) - 1:\n",
        "# \t\t\t#\ty_, _ = self.interpolator(semi_stable[i - 1].clone().detach(), semi_stable[i + 1].clone().detach())\n",
        "# \t\t\t#\tpixel_loss = pixel_loss + self.crit(semi_stable[i], y_)\n",
        "\n",
        "# \t\tif self.summary_writer is not None:\n",
        "# \t\t\tself.summary_writer.add_scalar('inner_loop_loss/pixel_loss', pixel_loss.item(), self.writer_counter)\n",
        "# \t\t\tself.summary_writer.add_scalar('inner_loop_loss/motion_loss', torch.abs(motion_between_gen_and_transformed).item(), self.writer_counter)\n",
        "# \t\t\tself.summary_writer.add_scalar('inner_loop_loss/perc_loss', vgg_loss.item(), self.writer_counter)\n",
        "# \t\t\tself.writer_counter += 1\n",
        "# \t\tcurr_loss = (pixel_loss) + 10*(torch.abs(motion_between_gen_and_transformed) + vgg_loss)\n",
        "\n",
        "# \t\treturn curr_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBBOEJaz32eW"
      },
      "source": [
        "###TRAIN + EVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utGfy25Idv4T"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "# os.chdir(os.path.dirname(__file__))\n",
        "# print(\"CWD:\", os.getcwd())\n",
        "\n",
        "# import argparse\n",
        "# import numpy as np\n",
        "# import cv2\n",
        "# from tqdm import tqdm\n",
        "# import natsort\n",
        "# import glob\n",
        "# import re\n",
        "# from collections import OrderedDict\n",
        "# import time\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import higher\n",
        "# import kornia as K\n",
        "# from tensorboardX import SummaryWriter\n",
        "# from datasets import MultiFramesDataset, create_data_loader\n",
        "# from losses import inner_loop_loss_perc_affine_another_try, outer_loop_loss_affine\n",
        "# import utils\n",
        "# import shutil\n",
        "\n",
        "# #### Some useless counters, cuz I'm lazy af...\n",
        "# INNER_ITER_COUNTER = 0\n",
        "# OUTER_ITER_COUNTER = 0\n",
        "# LBLS_GIVEN = True\n",
        "# USE_PREV_CROP_LOCS = False\n",
        "# PREV_CROP_H = 0\n",
        "# PREV_CROP_W = 0\n",
        "\n",
        "\n",
        "# def get_model(opts):\n",
        "#     if opts.model == \"DMBVS\":\n",
        "#         from models.model_enet import Generator as Model\n",
        "#         model = Model(in_channels=15, out_channels=3, residual_blocks=64)\n",
        "#         state_dict = torch.load(os.path.join(\"pretrained_models\", opts.ckp_name))['model']\n",
        "#         ##### create new OrderedDict that does not contain `module.`\n",
        "#         new_state_dict = OrderedDict()\n",
        "#         for k, v in state_dict.items():\n",
        "#             name = k[7:] # remove `module.`\n",
        "#             new_state_dict[name] = v\n",
        "#         # load params\n",
        "#         model.load_state_dict(new_state_dict)\n",
        "#     elif opts.model == 'difrint':\n",
        "#         from models.difrint import DIFNet_ours as Model\n",
        "#         model = Model()\n",
        "#         state_dict = torch.load('./pretrained_models/DIFNet2.pth')\n",
        "#         ##### create new OrderedDict that does not contain `module.`\n",
        "#         new_state_dict = OrderedDict()\n",
        "#         for k, v in state_dict.items():\n",
        "#             name = k[7:] # remove `module.`\n",
        "#             new_state_dict[name] = v\n",
        "#         # load params\n",
        "#         model.load_state_dict(new_state_dict)\n",
        "#         for i, child in enumerate(model.children()):\n",
        "#             if i < 2:\n",
        "#                 for param in child.parameters():\n",
        "#                     param.requires_grad = False\n",
        "#                 print(f'Freezed {child.__class__}')\n",
        "#             else:\n",
        "#                 pass\n",
        "#     else:\n",
        "#         raise Exception(\"Model not implemented: (%s)\" %opts.model)\n",
        "#     print('Loaded model state dict')\n",
        "#     return model\n",
        "\n",
        "\n",
        "# def get_data_loaders(opts):\n",
        "#     bc_dir = opts.data_dir\n",
        "#     opts_train = opts\n",
        "#     opts_train.data_dir = opts.train_data_dir\n",
        "#     train_dataset = MultiFramesDataset(mode= 'train', opts= opts_train, get_lbls= LBLS_GIVEN, prev_iter_counter= OUTER_ITER_COUNTER)\n",
        "#     opts.data_dir = bc_dir\n",
        "#     test_dataset = MultiFramesDataset(mode= \"val\", opts= opts)\n",
        "#     train_data_loader = create_data_loader(train_dataset, mode= \"train\",\n",
        "#         batch_size= opts.batch_size, threads= opts.threads, train_epoch_size= opts.train_epoch_size)\n",
        "#     test_data_loader = utils.create_data_loader(test_dataset, opts= opts, mode= \"val\")\n",
        "\n",
        "#     return train_data_loader, test_dataset # intentionally returning dataset instead of dataloader to gauge the performance on a single video [0]\n",
        "\n",
        "\n",
        "# def ip_gen_dif(frames, frames_op, start_id, device= \"cuda\", return_orig= False):\n",
        "#     if len(frames_op) == 0:\n",
        "#         frames_op.append(frames[start_id].clone())\n",
        "#     if device == \"cuda\":\n",
        "#         ip = torch.cat([frames_op[-1].cpu(),\n",
        "#             frames[start_id + 1],\n",
        "#             frames[start_id + 2]], 1).cuda()\n",
        "#         if return_orig:\n",
        "#             temp = frames[start_id + 1].clone().cuda()\n",
        "#     else:\n",
        "#         ip = torch.cat([frames_op[-1].cpu(),\n",
        "#             frames[start_id + 1],\n",
        "#             frames[start_id + 2]], 1)\n",
        "#         if return_orig:\n",
        "#             temp = frames[start_id + 1].clone()\n",
        "#     if return_orig:\n",
        "#         return ip, temp\n",
        "#     else:\n",
        "#         return ip\n",
        "\n",
        "\n",
        "# def ip_gen_dif_test(frames, frames_op, start_id, device= \"cuda\", patch_size= 320): #### For test time adaptation, does 5 adaptation steps on randomly cropped data and then we use the above one for full frame retrieval...\n",
        "#     b, c, h, w = frames[0].shape\n",
        "#     global USE_PREV_CROP_LOCS\n",
        "#     global PREV_CROP_H\n",
        "#     global PREV_CROP_W\n",
        "\n",
        "#     if not USE_PREV_CROP_LOCS:\n",
        "#         if h == patch_size:\n",
        "#             h_ = 0\n",
        "#         else:\n",
        "#             h_ = np.random.randint(0, h - patch_size)\n",
        "#         w_ = np.random.randint(0, w - patch_size)\n",
        "#         PREV_CROP_H = h_\n",
        "#         PREV_CROP_W = w_\n",
        "#         USE_PREV_CROP_LOCS = True\n",
        "#     else:\n",
        "#         h_ = PREV_CROP_H\n",
        "#         w_ = PREV_CROP_H\n",
        "\n",
        "#     if len(frames_op) == 0:\n",
        "#         frames_op.append(utils.tensor_crop(frames[start_id], h_, w_, patch_size))\n",
        "#     if device == \"cuda\":\n",
        "#         ip = torch.cat([frames_op[-1].cpu(),\n",
        "#             utils.tensor_crop(frames[start_id + 1], h_, w_, patch_size),\n",
        "#             utils.tensor_crop(frames[start_id + 2], h_, w_, patch_size)], 1).cuda()\n",
        "#         spt = utils.tensor_crop(frames[start_id + 1], h_, w_, patch_size).cuda()\n",
        "#     else:\n",
        "#         ip = torch.cat([frames_op[-1].cpu(),\n",
        "#             utils.tensor_crop(frames[start_id + 1], h_, w_, patch_size),\n",
        "#             utils.tensor_crop(frames[start_id + 2], h_, w_, patch_size)], 1)\n",
        "#         spt = utils.tensor_crop(frames[start_id + 1], h_, w_, patch_size)\n",
        "\n",
        "#     return ip, spt\n",
        "\n",
        "\n",
        "# def ip_gen_dif_for_outer(frames, frames_op, frames_lbl, start_id, device= \"cuda\", return_orig= True, resize= True):\n",
        "#     if len(frames_op) == 0:\n",
        "#         frames_op.append(frames[start_id])\n",
        "#     if device == \"cuda\":\n",
        "#         ip = torch.cat([frames_op[-1].cpu(),\n",
        "#             frames[start_id + 1],\n",
        "#             frames[start_id + 2]], 1).cuda()\n",
        "#         if return_orig:\n",
        "#             temp = frames_lbl[start_id + 1].clone().cuda()\n",
        "#     else:\n",
        "#         ip = torch.cat([frames_op[-1].cpu(),\n",
        "#             frames[start_id + 1],\n",
        "#             frames[start_id + 2]], 1)\n",
        "#         if return_orig:\n",
        "#             temp = frames_lbl[start_id + 1].clone()\n",
        "#     if return_orig:\n",
        "#         if resize:\n",
        "#             ip = nn.functional.interpolate(ip, (192, 192))\n",
        "#             temp = nn.functional.interpolate(temp, (192, 192))\n",
        "#         return ip, temp\n",
        "#     else:\n",
        "#         ip = nn.functional.interpolate(ip, (192, 192))\n",
        "#         return ip\n",
        "\n",
        "\n",
        "# def test_no_adaptation(db, net, device, L_in, epoch, opts):\n",
        "#     if not os.path.exists(\"./output_\" + opts.model_name + \"/\" + str(epoch) + \"/\"):\n",
        "#         os.makedirs(\"./output_\" + opts.model_name + \"/\" + str(epoch) + \"/\")\n",
        "\n",
        "#     if not os.path.exists(\"./output_\" + opts.model_name + \"/target/\"):\n",
        "#         os.makedirs(\"./output_\" + opts.model_name + \"/target/\")\n",
        "\n",
        "#     net.train()\n",
        "#     net.pwc.eval()\n",
        "\n",
        "#     final_op_list = []\n",
        "#     final_ip_list = []\n",
        "#     frame_pointer = 0\n",
        "#     interim_ops = []\n",
        "#     test_batch = db[0]\n",
        "#     frames = test_batch['X']\n",
        "\n",
        "#     pbar = tqdm(total= len(frames) - 5 - 5)\n",
        "#     while(frame_pointer < len(frames) - 5 - 5):\n",
        "#         with torch.no_grad():\n",
        "#             net.eval()\n",
        "#             ip_ = ip_gen_dif(frames, interim_ops, frame_pointer, return_orig= False)\n",
        "#             op_ = torch.clamp(net(ip_), min= 0.0, max= 1.0)\n",
        "#             interim_ops.append(op_.clone())\n",
        "#             final_op_list.append((utils.tensor2img(op_)*255).astype(np.uint8))\n",
        "#             final_ip_list.append((utils.tensor2img(frames[frame_pointer + 1])*255).astype(np.uint8))\n",
        "#             net.train()\n",
        "#         pbar.update(1)\n",
        "#         frame_pointer += 1\n",
        "#     pbar.close()\n",
        "#     for i in range(len(final_op_list)):\n",
        "#         cv2.imwrite(\"./output_\" + opts.model_name + \"/\" + str(epoch) + \"/\" + str(i) + \".png\", final_op_list[i])\n",
        "#     for i in range(len(final_ip_list)):\n",
        "#         cv2.imwrite(\"./output_\" + opts.model_name + \"/target/\" + str(i) + \".png\", final_ip_list[i])\n",
        "\n",
        "#     return final_op_list\n",
        "\n",
        "\n",
        "# def eval_no_adaptation(db, net, device, L_in, epoch, opts):\n",
        "#     if not os.path.exists(\"./output_\" + opts.model_name + \"/\" + str(epoch) + \"_no_adapt/\"):\n",
        "#         os.makedirs(\"./output_\" + opts.model_name + \"/\" + str(epoch) + \"_no_adapt/\")\n",
        "\n",
        "#     if not os.path.exists(\"./output_\" + opts.model_name + \"/target/\"):\n",
        "#         os.makedirs(\"./output_\" + opts.model_name + \"/target/\")\n",
        "\n",
        "#     net.train()\n",
        "#     net.pwc.eval()\n",
        "\n",
        "#     final_op_list = []\n",
        "#     final_ip_list = []\n",
        "#     frame_pointer = 0\n",
        "#     interim_ops = []\n",
        "#     test_batch = db[0]\n",
        "#     frames = test_batch['X']\n",
        "\n",
        "#     pbar = tqdm(total= len(frames) - 5 - 5)\n",
        "#     while(frame_pointer < len(frames) - 5 - 5):\n",
        "#         with torch.no_grad():\n",
        "#             net.eval()\n",
        "#             ip_ = ip_gen_dif(frames, interim_ops, frame_pointer, return_orig= False)\n",
        "#             op_ = torch.clamp(net(ip_), min= 0.0, max= 1.0)\n",
        "#             interim_ops.append(op_.clone())\n",
        "#             final_op_list.append((utils.tensor2img(op_)*255).astype(np.uint8))\n",
        "#             final_ip_list.append((utils.tensor2img(frames[frame_pointer + 1])*255).astype(np.uint8))\n",
        "#             net.train()\n",
        "#         pbar.update(1)\n",
        "#         frame_pointer += 1\n",
        "#     pbar.close()\n",
        "#     for i in range(len(final_op_list)):\n",
        "#         cv2.imwrite(\"./output_\" + opts.model_name + \"/\" + str(epoch) + \"_no_adapt/\" + str(i) + \".png\", final_op_list[i])\n",
        "#     for i in range(len(final_ip_list)):\n",
        "#         cv2.imwrite(\"./output_\" + opts.model_name + \"/target/\" + str(i) + \".png\", final_ip_list[i])\n",
        "\n",
        "#     return final_op_list\n",
        "\n",
        "\n",
        "# def test(db, net, device, L_in, epoch, opts):\n",
        "#     if not os.path.exists(\"./output_\" + opts.model_name + \"/\" + str(epoch) + \"/\"):\n",
        "#         os.makedirs(\"./output_\" + opts.model_name + \"/\" + str(epoch) + \"/\")\n",
        "#     net.train()\n",
        "#     net.pwc.eval()\n",
        "#     #n_test_iter = 20 ### generate 20 frames for test only\n",
        "\n",
        "#     final_op_list = []\n",
        "#     final_ip_list = []\n",
        "#     frame_pointer = 0\n",
        "\n",
        "#     test_batch = db[0]\n",
        "#     frames = test_batch['X']\n",
        "\n",
        "#     lr = 5e-7\n",
        "#     adaptation_number = 3\n",
        "#     optimizer = optim.Adam(net.parameters(), lr)\n",
        "#     pbar = tqdm(total= len(frames) - 5 - 5)\n",
        "#     global USE_PREV_CROP_LOCS\n",
        "#     while(frame_pointer < len(frames) - 5 - 5):\n",
        "#         interim_op = []\n",
        "#         USE_PREV_CROP_LOCS = False\n",
        "#         for _ in range(adaptation_number):\n",
        "#             optimizer.zero_grad()\n",
        "#             ip1, spt_ip1 = ip_gen_dif_test(frames, interim_op, start_id= frame_pointer, patch_size= opts.crop_size_adap)\n",
        "#             spt_op1 = torch.clamp(net(ip1), min= 0.0, max= 1.0)\n",
        "#             interim_op.append(spt_op1.clone())\n",
        "\n",
        "#             ip2, spt_ip2 = ip_gen_dif_test(frames, interim_op, start_id= frame_pointer + 1, patch_size= opts.crop_size_adap)\n",
        "#             spt_op2 = torch.clamp(net(ip2), min= 0.0, max= 1.0)\n",
        "#             interim_op.append(spt_op2.clone())\n",
        "\n",
        "#             ip3, spt_ip3 = ip_gen_dif_test(frames, interim_op, start_id= frame_pointer + 2, patch_size= opts.crop_size_adap)\n",
        "#             spt_op3 = torch.clamp(net(ip3), min= 0.0, max= 1.0)\n",
        "#             interim_op.append(spt_op3.clone())\n",
        "\n",
        "#             ip4, spt_ip4 = ip_gen_dif_test(frames, interim_op, start_id= frame_pointer + 3, patch_size= opts.crop_size_adap)\n",
        "#             spt_op4 = torch.clamp(net(ip4), min= 0.0, max= 1.0)\n",
        "#             interim_op.append(spt_op4.clone())\n",
        "\n",
        "#             ip5, spt_ip5 = ip_gen_dif_test(frames, interim_op, start_id= frame_pointer + 4, patch_size= opts.crop_size_adap)\n",
        "#             spt_op5 = torch.clamp(net(ip5), min= 0.0, max= 1.0)\n",
        "#             interim_op.append(spt_op5.clone())\n",
        "#             #spt_loss = L_in(spt_op1, spt_op2, spt_op3, spt_ip1, spt_ip2, spt_ip3)\n",
        "#             spt_loss = 0.5 * L_in([spt_op1, spt_op2, spt_op3, spt_op4, spt_op5], [spt_ip1, spt_ip2, spt_ip3, spt_ip4, spt_ip5])\n",
        "\n",
        "#             spt_loss.backward(retain_graph= True)\n",
        "#             optimizer.step()\n",
        "\n",
        "\n",
        "#         pbar.update(1)\n",
        "#         frame_pointer += 1\n",
        "#     pbar.close()\n",
        "#     frame_pointer = 0\n",
        "#     interim_ops = []\n",
        "#     while(frame_pointer < len(frames) - 5 - 5):\n",
        "#         with torch.no_grad():\n",
        "#             net.eval()\n",
        "#             ip_ = ip_gen_dif(frames, interim_ops, frame_pointer)\n",
        "#             op_ = torch.clamp(net(ip_), min= 0.0, max= 1.0)\n",
        "#             interim_ops.append(op_.clone())\n",
        "#             #frames[frame_pointer] = op_ #### frame recurrence...\n",
        "#             final_op_list.append((utils.tensor2img(op_)*255).astype(np.uint8))\n",
        "#             final_ip_list.append((utils.tensor2img(frames[frame_pointer + 1])*255).astype(np.uint8))\n",
        "#             frame_pointer += 1\n",
        "#         net.train()\n",
        "#     for i in range(len(final_op_list)):\n",
        "#         cv2.imwrite(\"./output_\" + opts.model_name + \"/\" + str(epoch) + \"/\" + str(i) + \".png\", final_op_list[i])\n",
        "\n",
        "#     return final_op_list\n",
        "\n",
        "\n",
        "# def train(data_loader, net, device, meta_opt, L_in, L_out, epoch, opts):\n",
        "#     net.train()\n",
        "#     net.pwc.eval() #### DIF ckpt contains PWCNet\n",
        "\n",
        "#     global INNER_ITER_COUNTER\n",
        "#     global OUTER_ITER_COUNTER\n",
        "\n",
        "#     if epoch > opts.lr_offset:\n",
        "#         current_lr = utils.learning_rate_decay_simple(opts, epoch)\n",
        "#     else:\n",
        "#         current_lr = opts.lr_init\n",
        "#     for param_group in meta_opt.param_groups:\n",
        "#         param_group['lr'] = current_lr\n",
        "\n",
        "#     loss_writer.add_scalar(\"hyperparameters/LR\", current_lr, epoch)\n",
        "\n",
        "#     for batch_idx, batch in enumerate(data_loader, 1):\n",
        "#         start_time = time.time()\n",
        "#         frames = batch['X']\n",
        "#         if LBLS_GIVEN:\n",
        "#             frames_lbl = batch['Y']\n",
        "\n",
        "#         frame_inds = [0, 3, 6, 9, 12] #### for 5 (with 3 disjoint samples) inner loop iterations\n",
        "#         n_inner_iter = 1 #len(frame_inds) #for doing 1 inner loop optimization...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         inner_opt = torch.optim.Adam(net.parameters(), lr=current_lr)\n",
        "\n",
        "\n",
        "#         meta_opt.zero_grad()\n",
        "#         for i in range(1):\n",
        "#             with higher.innerloop_ctx(\n",
        "#                 net, inner_opt, copy_initial_weights=False\n",
        "#             ) as (fnet, diffopt):\n",
        "#                 for ind in range(n_inner_iter):\n",
        "#                     interim_op = []\n",
        "#                     ip1, unstab1 = ip_gen_dif(frames, interim_op, start_id= frame_inds[ind], return_orig= True)\n",
        "#                     spt_op1 = torch.clamp(fnet(ip1), min= 0.0, max= 1.0)\n",
        "#                     interim_op.append(spt_op1.clone())\n",
        "\n",
        "#                     ip2, unstab2 = ip_gen_dif(frames, interim_op, start_id= frame_inds[ind] + 1, return_orig= True)\n",
        "#                     spt_op2 = torch.clamp(fnet(ip2), min= 0.0, max= 1.0)\n",
        "#                     interim_op.append(spt_op2.clone())\n",
        "\n",
        "#                     ip3, unstab3 = ip_gen_dif(frames, interim_op, start_id= frame_inds[ind] + 2, return_orig= True)\n",
        "#                     spt_op3 = torch.clamp(fnet(ip3), min= 0.0, max= 1.0)\n",
        "#                     interim_op.append(spt_op3.clone())\n",
        "\n",
        "#                     ip4, unstab4 = ip_gen_dif(frames, interim_op, start_id= frame_inds[ind] + 3, return_orig= True)\n",
        "#                     spt_op4 = torch.clamp(fnet(ip4), min= 0.0, max= 1.0)\n",
        "#                     interim_op.append(spt_op4.clone())\n",
        "\n",
        "#                     ip5, unstab5 = ip_gen_dif(frames, interim_op, start_id= frame_inds[ind] + 4, return_orig= True)\n",
        "#                     spt_op5 = torch.clamp(fnet(ip5), min= 0.0, max= 1.0)\n",
        "#                     interim_op.append(spt_op5.clone())\n",
        "\n",
        "\n",
        "#                     spt_loss = L_in([spt_op1, spt_op2, spt_op3, spt_op4, spt_op5], [unstab1, unstab2, unstab3, unstab4, unstab5])\n",
        "#                     diffopt.step(spt_loss)\n",
        "#                     loss_writer.add_scalar(\"comb_losses/InnerLoopLoss\", spt_loss.item(), INNER_ITER_COUNTER)\n",
        "#                     INNER_ITER_COUNTER += 1\n",
        "#                     print(\"Epoch:\", epoch, \"Outer_iter:\", batch_idx, \"Inner_iter:\", ind, \"L_in:\", spt_loss.item(), \"({}/{})\".format(batch_idx, len(data_loader)))\n",
        "\n",
        "#                 op_qry = []\n",
        "#                 stab_qry = []\n",
        "\n",
        "#                 interim_ops = []\n",
        "#                 for q in range(6):\n",
        "#                     id_for_qry = frame_inds[-1] + 3 + q\n",
        "#                     if LBLS_GIVEN:\n",
        "#                         ip_qry, stab = ip_gen_dif_for_outer(frames, interim_ops, frames_lbl, start_id= id_for_qry)\n",
        "#                     else:\n",
        "#                         ip_qry, stab = ip_gen_dif_for_outer(frames, interim_ops, frames, start_id= id_for_qry)\n",
        "#                     op_qry.append(torch.clamp(fnet(ip_qry), min= 0.0, max= 1.0))\n",
        "#                     interim_ops.append(op_qry[-1].clone())\n",
        "#                     stab_qry.append(stab)\n",
        "\n",
        "#                 qry_loss = L_out(op_qry, stab_qry)\n",
        "\n",
        "#                 loss_writer.add_scalar(\"comb_losses/OuterLoopLoss\", qry_loss.item(), OUTER_ITER_COUNTER)\n",
        "#                 OUTER_ITER_COUNTER += 1\n",
        "#                 qry_loss.backward()\n",
        "#                 print(\"Epoch:\", epoch, \"Outer_iter:\", batch_idx, \"Inner_iter:\", ind, \"L_out:\", qry_loss.item(), \"({}/{})\".format(batch_idx, len(data_loader)))\n",
        "\n",
        "#             meta_opt.step()\n",
        "#             torch.cuda.empty_cache()\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "#     return net, meta_opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbyvwRsw26Oe"
      },
      "outputs": [],
      "source": [
        "# parser = argparse.ArgumentParser(description=\"Meta Stabilization\")\n",
        "\n",
        "# ### model options\n",
        "# parser.add_argument('-model',           type=str,     default=\"difrint\",                        help='Model to use')\n",
        "# parser.add_argument('-ckp_name',        type=str,     default=\"best_stab_vf_v1.h5\",             help='checkpoint_name')\n",
        "# parser.add_argument('-model_name',      type=str,     default='dif_after_1_4',                  help='path to save model')\n",
        "\n",
        "# ### dataset options\n",
        "# parser.add_argument('-train_data_dir',  type=str,     default='../DeepStab/unstable_frames/',   help='path to train data folder')\n",
        "# parser.add_argument('-train_lbl_dir',  type=str,     default='../DeepStab/stable_frames/',      help='path to train data folder')\n",
        "# parser.add_argument('-data_dir',        type=str,     default='../DeepStab/unstable_frames/',   help='path to test data folder')#### change to the folder containing NUS Dataset videos\n",
        "# parser.add_argument('-checkpoint_dir',  type=str,     default='checkpoints',                    help='path to checkpoint folder')\n",
        "# parser.add_argument('-crop_size',       type=int,     default=192,                              help='patch size')\n",
        "# parser.add_argument('-crop_size_adap',  type=int,     default=320,                              help='adaptation patch size')\n",
        "# parser.add_argument('-geometry_aug',    type=int,     default=0,                                help='geometry augmentation (rotation, scaling, flipping)')\n",
        "# parser.add_argument('-order_aug',       type=int,     default=1,                                help='temporal ordering augmentation')\n",
        "# parser.add_argument('-scale_min',       type=float,   default=0.5,                              help='min scaling factor')\n",
        "# parser.add_argument('-scale_max',       type=float,   default=2.0,                              help='max scaling factor')\n",
        "# parser.add_argument('-sample_frames',   type=int,     default=63,                               help='#frames for training') #### Min number of frames in one of the deep stab scenes are 67...\n",
        "\n",
        "# ### training options\n",
        "# parser.add_argument('-solver',          type=str,     default=\"ADAM\",                           choices=[\"SGD\", \"ADAM\"],   help=\"optimizer\")\n",
        "# parser.add_argument('-momentum',        type=float,   default=0.9,                              help='momentum for SGD')\n",
        "# parser.add_argument('-beta1',           type=float,   default=0.9,                              help='beta1 for ADAM')\n",
        "# parser.add_argument('-beta2',           type=float,   default=0.999,                            help='beta2 for ADAM')\n",
        "# parser.add_argument('-weight_decay',    type=float,   default=0,                                help='weight decay')\n",
        "# parser.add_argument('-batch_size',      type=int,     default=4,                                help='training batch size')\n",
        "# parser.add_argument('-train_epoch_size',type=int,     default=200,                              help='train epoch size')\n",
        "# parser.add_argument('-valid_epoch_size',type=int,     default=100,                              help='valid epoch size')\n",
        "# parser.add_argument('-epoch_max',       type=int,     default=1000,                             help='max #epochs')\n",
        "\n",
        "\n",
        "# ### learning rate options\n",
        "# parser.add_argument('-lr_init',         type=float,   default=1e-5,                             help='initial learning Rate')\n",
        "# parser.add_argument('-lr_offset',       type=int,     default=10,                               help='epoch to start learning rate drop [-1 = no drop]')\n",
        "# parser.add_argument('-lr_step',         type=int,     default=1,                                help='step size (epoch) to drop learning rate')\n",
        "# parser.add_argument('-lr_drop',         type=float,   default=0.99,                             help='learning rate drop ratio')\n",
        "# parser.add_argument('-lr_min_m',        type=float,   default=0.001,                            help='minimal learning Rate multiplier (lr >= lr_init * lr_min)')\n",
        "\n",
        "\n",
        "# ### other options\n",
        "# parser.add_argument('-loss',            type=str,     default=\"L1\",                             help=\"Loss [Options: L1, L2]\")\n",
        "# parser.add_argument('-seed',            type=int,     default=9487,                             help='random seed to use')\n",
        "# parser.add_argument('-threads',         type=int,     default=8,                                help='number of threads for data loader to use')\n",
        "# parser.add_argument('-suffix',          type=str,     default='',                               help='name suffix')\n",
        "# parser.add_argument('-gpu',             type=int,     default=1,                                help='gpu device id')\n",
        "# parser.add_argument('-cpu',             action='store_true',                                    help='use cpu?')\n",
        "\n",
        "# opts = parser.parse_args()\n",
        "\n",
        "\n",
        "# #### adjust the training data folder (only if you didn't set it earlier....)\n",
        "# opts.train_data_dir = \"../DeepStab/unstable_frames/\"\n",
        "\n",
        "\n",
        "# opts.cuda = (opts.cpu != True)\n",
        "# opts.lr_min = opts.lr_init * opts.lr_min_m\n",
        "\n",
        "# ### default model name\n",
        "# if opts.model_name == 'none':\n",
        "#     opts.model_name = \"%s_%s\" %(opts.model, opts.ckp_name)\n",
        "\n",
        "# if opts.suffix != \"\":\n",
        "#     opts.model_name += \"_%s\" %opts.suffix\n",
        "\n",
        "\n",
        "# opts.size_multiplier = 2 ** 6 ## Inputs to FlowNet need to be divided by 64\n",
        "\n",
        "# print(opts)\n",
        "\n",
        "# ### model saving directory\n",
        "# opts.model_dir = os.path.join(opts.checkpoint_dir, opts.model_name)\n",
        "# print(\"========================================================\")\n",
        "# print(\"===> Save model to %s\" %opts.model_dir)\n",
        "# print(\"========================================================\")\n",
        "# if not os.path.isdir(opts.model_dir):\n",
        "#     os.makedirs(opts.model_dir)\n",
        "\n",
        "# ### initialize model\n",
        "# print('===> Initializing model from %s...' %opts.model)\n",
        "# model = get_model(opts)\n",
        "\n",
        "# ### Meta optimizer...\n",
        "# ### initialize optimizer\n",
        "# if opts.solver == 'SGD':\n",
        "#     meta_opt = optim.SGD(model.parameters(), lr=opts.lr_init, momentum=opts.momentum, weight_decay=opts.weight_decay)\n",
        "# elif opts.solver == 'ADAM':\n",
        "#     meta_opt = optim.Adam(model.parameters(), lr=opts.lr_init, weight_decay=opts.weight_decay, betas=(opts.beta1, opts.beta2))\n",
        "# else:\n",
        "#     raise Exception(\"Not supported solver (%s)\" %opts.solver)\n",
        "\n",
        "# if opts.loss == 'L2':\n",
        "#     criterion = nn.MSELoss(size_average=True)\n",
        "# elif opts.loss == 'L1':\n",
        "#     criterion = nn.L1Loss(size_average=True)\n",
        "# else:\n",
        "#     raise Exception(\"Unsupported criterion %s\" %opts.loss)\n",
        "\n",
        "# num_params = utils.count_network_parameters(model)\n",
        "\n",
        "\n",
        "# ### resume latest model\n",
        "# name_list = glob.glob(os.path.join(opts.model_dir, \"model_epoch_*.pth\"))\n",
        "# epoch_st = 0\n",
        "\n",
        "# if len(name_list) > 0:\n",
        "#     epoch_list = []\n",
        "#     for name in name_list:\n",
        "#         s = re.findall(r'\\d+', os.path.basename(name))[0]\n",
        "#         epoch_list.append(int(s))\n",
        "\n",
        "#     epoch_list.sort()\n",
        "#     epoch_st = epoch_list[-1]\n",
        "\n",
        "\n",
        "# if epoch_st > 0:\n",
        "\n",
        "#     print('=====================================================================')\n",
        "#     print('===> Resuming model from epoch %d' %epoch_st)\n",
        "#     print('=====================================================================')\n",
        "\n",
        "#     ### resume latest model and solver\n",
        "#     model, meta_opt = utils.load_model(model, meta_opt, opts, epoch_st)\n",
        "\n",
        "# else:\n",
        "#     ### save epoch 0\n",
        "#     utils.save_model(model, meta_opt, opts, 0)\n",
        "\n",
        "\n",
        "# print('\\n=====================================================================')\n",
        "# print(\"===> Model has %d parameters\" %num_params)\n",
        "# print('=====================================================================')\n",
        "\n",
        "\n",
        "# ### initialize loss writer\n",
        "# loss_dir = os.path.join(opts.model_dir, 'loss')\n",
        "# loss_writer = SummaryWriter(loss_dir)\n",
        "\n",
        "\n",
        "# #Will pass it to the loss definition\n",
        "\n",
        "# ### Load pretrained GFlowNet\n",
        "# GFlowNet = GPWC().eval()\n",
        "# for param in GFlowNet.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# ### convert to GPU\n",
        "# device = torch.device(\"cuda\" if opts.cuda else \"cpu\")\n",
        "# model = model.to(device)\n",
        "# GFlowNet = GFlowNet.to(device)\n",
        "\n",
        "# ### Define losses here and pass flownet and interpolator to losses\n",
        "# L_in = inner_loop_loss_perc_affine_another_try(criterion, None, GFlowNet, summary_writer= loss_writer)\n",
        "# L_out = outer_loop_loss_affine(GFlowNet, None, summary_writer= loss_writer)\n",
        "\n",
        "# train_data_loader, test_data_loader = get_data_loaders(opts)\n",
        "\n",
        "# #print(\"Testing no adaptation...\")\n",
        "# #_ = test_no_adaptation(test_data_loader, model, device, L_in, 0, opts)\n",
        "# torch.cuda.empty_cache()\n",
        "# for epoch in range(1, opts.epoch_max):\n",
        "#     train_data_loader, test_data_loader = get_data_loaders(opts)\n",
        "#     model, meta_opt = train(train_data_loader, model, device, meta_opt, L_in, L_out, epoch, opts)\n",
        "#     torch.cuda.empty_cache()\n",
        "#     print(\"Evaluating without adaptation...\")\n",
        "#     _ = eval_no_adaptation(test_data_loader, model, device, L_in, epoch, opts)\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "#     #### Making a copy of the model weights to do adaptation for performance check, can also use deepcopy...\n",
        "#     model_for_adaptation = get_model(opts).to(device)\n",
        "#     model_for_adaptation.load_state_dict(model.state_dict())\n",
        "#     print(\"Testing with adaptation...\")\n",
        "#     _ = test(test_data_loader, model_for_adaptation, device, L_in, epoch, opts)\n",
        "#     del model_for_adaptation\n",
        "#     utils.save_model(model, meta_opt, opts, epoch)\n",
        "#     torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4t22vUJ3_DV"
      },
      "source": [
        "###TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6DVmEZ18CDo"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def scale_video(input_path, output_path, target_size=(256, 256)):\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or 'XVID' depending on your system\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 30.0, target_size)  # 30.0 is frames per second\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        resized_frame = cv2.resize(frame, target_size)\n",
        "        out.write(resized_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Itu4tS7z3-yv"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "def calculate_stability_score(original_video_path, stabilized_video_path):\n",
        "    cap_orig = cv2.VideoCapture(original_video_path)\n",
        "    cap_stab = cv2.VideoCapture(stabilized_video_path)\n",
        "\n",
        "    stability_scores = []\n",
        "\n",
        "    while True:\n",
        "        ret_orig, frame_orig = cap_orig.read()\n",
        "        ret_stab, frame_stab = cap_stab.read()\n",
        "\n",
        "        if not ret_orig or not ret_stab:\n",
        "            break\n",
        "\n",
        "        frame_orig_gray = cv2.cvtColor(frame_orig, cv2.COLOR_BGR2GRAY)\n",
        "        frame_stab_gray = cv2.cvtColor(frame_stab, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        score, _ = ssim(frame_orig_gray, frame_stab_gray, full=True)\n",
        "        stability_scores.append(score)\n",
        "\n",
        "    cap_orig.release()\n",
        "    cap_stab.release()\n",
        "\n",
        "    return np.mean(stability_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t573JOMECdhu"
      },
      "source": [
        "####So sánh video đầu ra với stable video dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G44ugLPVBfIG"
      },
      "outputs": [],
      "source": [
        "input_video_path = '/content/DeepStab/stable/1.avi'\n",
        "output_video_path = '/content/scaled_video.avi'\n",
        "scale_video(input_video_path, output_video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_wXAgf9Bjhb",
        "outputId": "5a11582c-3bc4-4d85-bf94-995c2ee8b2fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stability Score: 0.3257793336397868\n"
          ]
        }
      ],
      "source": [
        "original_video = '/content/scaled_video.avi'\n",
        "stabilized_video = '/content/stable_result.avi'\n",
        "stability_score = calculate_stability_score(original_video, stabilized_video)\n",
        "print(f'Stability Score: {stability_score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_TU5Se1CqFD"
      },
      "source": [
        "####So sánh video đầu ra với unstable video dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm6c2IuIBxvu"
      },
      "outputs": [],
      "source": [
        "input_video_path = '/content/DeepStab/unstable/1.avi'\n",
        "output_video_path = '/content/unstable_scaled_video.avi'\n",
        "scale_video(input_video_path, output_video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwQvgFnYBplG",
        "outputId": "3d8484d6-30af-4715-a29e-16745a58d751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stability Score: 0.6711860836525776\n"
          ]
        }
      ],
      "source": [
        "original_video = '/content/unstable_scaled_video.avi'\n",
        "stabilized_video = '/content/stable_result.avi'\n",
        "stability_score = calculate_stability_score(original_video, stabilized_video)\n",
        "print(f'Stability Score: {stability_score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHNlRYuTCv0j"
      },
      "source": [
        "###COnvert rt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ5nYJrPCRh6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.quantization\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "\n",
        "# Assuming `model` is your PyTorch model\n",
        "model = CAIN(training=False, depth=3)  # Example model\n",
        "model.eval()\n",
        "\n",
        "# Apply dynamic quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Save the quantized model\n",
        "torch.save(quantized_model.state_dict(), \"quantized_model.pth\")\n",
        "#Convert the Quantized Model to ONNX\n",
        "# Đầu vào giả cho mô hình CAIN\n",
        "dummy_input_cain1 = torch.randn(1, 3, 256, 256).cuda()  # Điều chỉnh kích thước nếu cần\n",
        "dummy_input_cain2 = torch.randn(1, 3, 256, 256).cuda()  # Đầu vào thứ hai cho CAIN\n",
        "\n",
        "# Xuất mô hình CAIN sang ONNX\n",
        "torch.onnx.export(\n",
        "    cain,\n",
        "    (dummy_input_cain1, dummy_input_cain2),  # Đầu vào giả\n",
        "    \"cain_model.onnx\",  # Tên tệp xuất ra\n",
        "    opset_version=11,\n",
        "    input_names=['input1', 'input2'],\n",
        "    output_names=['output']\n",
        ")\n",
        "print('Mô hình CAIN đã được xuất sang định dạng ONNX.')\n",
        "\n",
        "# Đầu vào giả cho mô hình ResNet\n",
        "dummy_input_resnet = torch.randn(1, 15, 256, 256).cuda()  # Điều chỉnh kích thước nếu cần\n",
        "\n",
        "# Xuất mô hình ResNet sang ONNX\n",
        "torch.onnx.export(\n",
        "    resnet,\n",
        "    dummy_input_resnet,  # Đầu vào giả\n",
        "    \"resnet_model.onnx\",  # Tên tệp xuất ra\n",
        "    opset_version=11,\n",
        "    input_names=['input'],\n",
        "    output_names=['output']\n",
        ")\n",
        "print('Mô hình ResNet đã được xuất sang định dạng ONNX.')\n",
        "\n",
        "#trtexec --onnx=quantized_model.onnx --saveEngine=quantized_model.trt --fp16 (down to fp32->fp16)\n",
        "trtexec --onnx=quantized_model.onnx --saveEngine=quantized_model.trt --fp16\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "p-9yPVh27IXV",
        "sQpm7UA44zHV",
        "CfESiJNP3wEm",
        "aBBOEJaz32eW",
        "m4t22vUJ3_DV",
        "t573JOMECdhu",
        "N_TU5Se1CqFD",
        "nHNlRYuTCv0j"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
